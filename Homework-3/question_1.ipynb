{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e77c834f-f4c3-4f2a-b824-3861d0d8e062",
   "metadata": {},
   "source": [
    "# Question 1: Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2d3be37-b340-4f16-87fa-dc03d30ce661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import product\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import cross_entropy\n",
    "from transformers import AdamW, AutoTokenizer, BertModel\n",
    "from utils.data_loader import MultipleChoiceDataloader, read_file, read_json_data\n",
    "from utils.train_classifier import train_loop\n",
    "from utils.valid_classifier import valid_loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bea21d5c-8a1c-4f4e-8e99-ef601849e5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "PRINT_EVERY = 100\n",
    "BATCH_SIZE = 150"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5e8a98-805a-4f4c-8325-ab63c955be35",
   "metadata": {},
   "source": [
    "## Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "624ad89a-6d63-4acc-ab3a-704319a4bf6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file_name = \"data/train_complete.jsonl\"\n",
    "dev_file_name = \"data/dev_complete.jsonl\"\n",
    "test_file_name = \"data/test_complete.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ef24a30b-fef1-4813-9376-1b18e071936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_json = read_file(train_file_name)\n",
    "dev_json = read_file(dev_file_name)\n",
    "test_json = read_file(test_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "104fdc42-5abb-48b9-af51-67ad8781bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = read_json_data(train_json)\n",
    "dev_dataset = read_json_data(dev_json, permute=False)\n",
    "test_dataset = read_json_data(test_json, permute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c76036c-1caf-45c6-87a6-415c03f08f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = MultipleChoiceDataloader(train_dataset, batch_size=BATCH_SIZE)\n",
    "dev_dataloader = MultipleChoiceDataloader(dev_dataset, batch_size=BATCH_SIZE)\n",
    "test_dataloader = MultipleChoiceDataloader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0263ec5e-2b82-42f0-9114-d769bf3741fc",
   "metadata": {},
   "source": [
    "Sample sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef34f498-4931-4d69-97d7-9cb1f158b955",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS] the sun is the source of energy for physical cycles on Earth [SEP] The sun is responsible for puppies learning new tricks [SEP] children growing up and getting old [SEP] flowers wilting in a vase [SEP] plants sprouting, blooming and wilting [END]\n",
      "tensor([0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "for text, label in train_dataloader:\n",
    "    print(text[0])\n",
    "    print(label[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9532b87-e443-4601-a2f0-d9137661cec3",
   "metadata": {},
   "source": [
    "The answers and stem are encoded as a single sequence. Everything is separated using `[SEP]`; so our format is `[CLS] stem [SEP] option [SEP] option [SEP] option [SEP] option [END]`. For example:\n",
    "\n",
    "```\n",
    "[CLS] the sun is the source of energy for physical cycles on Earth [SEP] The sun is responsible for puppies learning new tricks [SEP] children growing up and getting old [SEP] flowers wilting in a vase [SEP] plants sprouting, blooming and wilting [END]\n",
    "```\n",
    "\n",
    "During training, we're permuting the options. This way we get an artifically larger dataset (a form of bootstrap), while also ensuring the transfomer doesn't memorize the options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15406b07-7f31-4680-94ae-1702089012f7",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d3ce429-6faa-4be9-bb50-8ef0bfc60a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "linear = nn.Linear(768, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e6ab44-ae63-4750-9f45-507b97358908",
   "metadata": {},
   "source": [
    "* We're using `AdamW` instead of the standard `Adam` for both the BERT model and the linear layer together.\n",
    "* AdamW is better suited for  BERT due to its handling of weight decay, which it applies differently from traditional L2 regularization, directly influencing the gradients rather than just the weight update step.\n",
    "* We're also varying our probability of drop out, so our model doesn't memorize the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d23be91c-9db5-4bf0-aeb6-956a203eac8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    \"learning_rate\": [1e-5, 3e-5, 5e-5],\n",
    "    \"weight_decay\": [0.01, 0.05],\n",
    "    \"dropout_prob\": [0.1, 0.3],\n",
    "}\n",
    "param_combinations = list(product(*hyperparams.values()))\n",
    "param_names = list(hyperparams.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc5b943a-37a9-4f1a-9559-51575e86ae19",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "732c6b1d-313a-4f58-9a94-5bacf217105f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d137df2-611c-4494-9ba8-044893a00844",
   "metadata": {},
   "source": [
    "Do not run the code snippet below unless you want to train all the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896e3d8b-ecb9-4b3e-80d5-0ad9a7f47b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with parameters: {'learning_rate': 1e-05, 'weight_decay': 0.01, 'dropout_prob': 0.1}\n",
      "Epoch 1/5\n",
      "Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/venv/lib/python3.10/site-packages/transformers/optimization.py:521: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 100/794: Loss = 1.3926, Accuracy = 0.2466, Time = 52.27s\n",
      "Batch 200/794: Loss = 1.3890, Accuracy = 0.2659, Time = 53.84s\n",
      "Batch 300/794: Loss = 1.3366, Accuracy = 0.3291, Time = 52.57s\n",
      "Batch 400/794: Loss = 1.2534, Accuracy = 0.4041, Time = 53.73s\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for values in param_combinations:\n",
    "    params = dict(zip(param_names, values))\n",
    "    print(\"Training with parameters:\", params)\n",
    "\n",
    "    model.config.hidden_dropout_prob = params[\"dropout_prob\"]\n",
    "    model.config.attention_probs_dropout_prob = params[\"dropout_prob\"]\n",
    "    optimizer = AdamW(\n",
    "        [{\"params\": model.parameters()}, {\"params\": linear.parameters()}],\n",
    "        lr=params[\"learning_rate\"],\n",
    "        weight_decay=params[\"weight_decay\"],\n",
    "    )\n",
    "    model.to(device)\n",
    "    linear.to(device)\n",
    "\n",
    "    epoch_train_losses = []\n",
    "    epoch_train_accuracies = []\n",
    "    epoch_valid_losses = []\n",
    "    epoch_valid_accuracies = []\n",
    "\n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        print(\"Training...\")\n",
    "        train_dataloader.shuffle_data()\n",
    "        train_metrics = train_loop(\n",
    "            train_dataloader,\n",
    "            tokenizer,\n",
    "            model,\n",
    "            linear,\n",
    "            loss_fn,\n",
    "            optimizer,\n",
    "            device,\n",
    "            PRINT_EVERY,\n",
    "        )\n",
    "        epoch_train_losses.append(train_metrics[0])\n",
    "        epoch_train_accuracies.append(train_metrics[1])\n",
    "\n",
    "        print(\"Validating...\")\n",
    "        valid_metrics = valid_loop(\n",
    "            dev_dataloader, tokenizer, model, linear, loss_fn, device, PRINT_EVERY\n",
    "        )\n",
    "        epoch_valid_losses.append(valid_metrics[0])\n",
    "        epoch_valid_accuracies.append(valid_metrics[1])\n",
    "\n",
    "        results.append(\n",
    "            {\n",
    "                **params,\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_metrics[0],\n",
    "                \"train_accuracy\": train_metrics[1],\n",
    "                \"valid_loss\": valid_metrics[0],\n",
    "                \"valid_accuracy\": valid_metrics[1],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epoch_train_losses, label=\"Train Loss\")\n",
    "    plt.plot(epoch_valid_losses, label=\"Valid Loss\")\n",
    "    plt.title(\"Training and Validation Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epoch_train_accuracies, label=\"Train Accuracy\")\n",
    "    plt.plot(epoch_valid_accuracies, label=\"Valid Accuracy\")\n",
    "    plt.title(\"Training and Validation Accuracy\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    param_info = \"\\n\".join([f\"{key}: {val}\" for key, val in params.items()])\n",
    "    plt.figtext(\n",
    "        0.5,\n",
    "        0.01,\n",
    "        param_info,\n",
    "        ha=\"center\",\n",
    "        fontsize=10,\n",
    "        bbox={\"facecolor\": \"white\", \"alpha\": 0.5, \"pad\": 5},\n",
    "    )\n",
    "\n",
    "    plot_path = os.path.join(\n",
    "        image_directory,\n",
    "        f'epoch_metrics_{params[\"learning_rate\"]}_{params[\"weight_decay\"]}_{params[\"dropout_prob\"]}.png',\n",
    "    )\n",
    "    plt.savefig(plot_path, bbox_inches=\"tight\")\n",
    "    plt.close()\n",
    "    print(f\"Saved plot to {plot_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39234c70-50a1-4941-8cc1-50560c430648",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"/results/hyperparam_results.csv\", index=False)\n",
    "print(\"Saved hyperparameter tuning results to '/results/hyperparam_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7a8e0-d312-410f-865c-b94a1329944d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
