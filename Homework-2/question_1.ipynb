{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1608669c-eb69-426c-be99-15c9d9812965",
   "metadata": {},
   "source": [
    "# Question 1: GPT-2 style autoregressive language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a4d68-5b42-4fe0-8006-7e0c7f810a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext.data as data\n",
    "from models.attention import MultiHeadAttention\n",
    "from models.embedder import Embedder, FeedForward, Norm, PositionalEncoder, get_clones\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01dae61-429f-4f97-bf53-7bd920faa875",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df5176-9027-4f32-a095-c9cf374eea42",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "* the first attention mechanism in each decoder layer is self-attention\n",
    "    * operates solely within the decoder\n",
    "    * focuses on the target sequence\n",
    "    * we're going to keep this\n",
    "* the second attention mechanism is the encoder-decoder attention\n",
    "    * this is \"cross-attention\"\n",
    "    * One of the sequences serves as a query input, while the other as a key and value inputs\n",
    "* in a GPT-2-style autoregressive model\n",
    "    * the encoder-decoder attention is not used because there is no distinct encoder phase\n",
    "    * the model is solely based on a stacked decoder-like architecture with self-attention layers\n",
    "    * this only attends to previous positions in the input sequence (using a no-peak mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5104169e-9892-46e8-a62b-5ef1bab5e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.attn_1 = MultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "846b2ac0-4c97-43bd-b8f6-a46a9cb9e386",
   "metadata": {},
   "source": [
    "Since there's no longer an encoder output (`e_outputs`), we can remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c29c7b6-9ede-4de2-92bf-01b772a7c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "    def forward(self, trg, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e24f7-c328-47a5-b169-c7a6c9310e3e",
   "metadata": {},
   "source": [
    "## Transformer\n",
    "\n",
    "* the Transformer below essentially functions like an autoregressive language model, similar to GPT-2.\n",
    "* This model doesn't have an encoder and uses only a decoder.\n",
    "* we're now using a single vocab size since we don't have a source and target vocab.\n",
    "\n",
    "```python\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab, trg_vocab, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(src_vocab, d_model, N, heads, dropout) # remove this\n",
    "        self.decoder = Decoder(trg_vocab, d_model, N, heads, dropout)\n",
    "        self.out = nn.Linear(d_model, trg_vocab)\n",
    "\n",
    "    def forward(self, src, trg, src_mask, trg_mask): # now only target vocab\n",
    "        e_outputs = self.encoder(src, src_mask)\n",
    "        # print(\"DECODER\")\n",
    "        d_output = self.decoder(trg, e_outputs, src_mask, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76eedc1-1510-4165-8dd0-b55df5b7805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "        self.decoder = Decoder(vocab_size, d_model, N, heads, dropout)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, trg, trg_mask):\n",
    "        d_output = self.decoder(trg, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ab07d-4c13-4869-b51e-45951d93f764",
   "metadata": {},
   "source": [
    "## No-Peak Mask\n",
    "\n",
    "* `nopeak_mask`: create an upper triangular matrix using `torch.triu()`\n",
    "* `create_src_mask`: create a mask to remove padding tokens from the source sequence so they don't contribute to training\n",
    "* we `&` these masks to not only padding tokens but also future tokens in the target sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5209633-9eb5-49bf-bdac-3b1d2fc69593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size, device):\n",
    "    np_mask = torch.triu(torch.ones((1, size, size), device=device), 1).bool()\n",
    "    return np_mask\n",
    "\n",
    "\n",
    "def create_src_mask(src, pad_token_id, device):\n",
    "    src_mask = (src != pad_token_id).unsqueeze(-2)\n",
    "    return src_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae92f31-78aa-4ccc-9002-bb588cb0f656",
   "metadata": {},
   "source": [
    "## Data Feeder\n",
    "\n",
    "* preparing a dataset and an iterator for training using torchtext\n",
    "* Source: https://gmihaila.github.io/ml_things/tutorial_notebooks/pytorchtext_bucketiterator/\n",
    "\n",
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d4a125-4183-4c80-9ea4-65cf66a42a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f318f68-eacb-48a9-beb0-6574b64b00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER.add_special_tokens(\n",
    "    {\"pad_token\": \"[PAD]\", \"bos_token\": \"<sos>\", \"eos_token\": \"<eos>\"}\n",
    ")\n",
    "TOKENIZER.pad_token_id, TOKENIZER.bos_token_id, TOKENIZER.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c490266-f504-4f40-8bd7-529c9d165cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = TOKENIZER.encode(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,  # sequence length\n",
    "        truncation=True,\n",
    "    )\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e4188-f0a3-4da0-b0af-89609658bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    examples = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                src_line = line.strip()\n",
    "                examples.append(data.Example.fromlist([src_line], [(\"src\", SRC)]))\n",
    "    dataset = data.Dataset(examples, [(\"src\", SRC)])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2a94f-e0b7-4694-a8b1-e366b727b623",
   "metadata": {},
   "source": [
    "### BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad1ef0-ee50-4ac4-8543-52cd09150224",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(\n",
    "    use_vocab=False,\n",
    "    tokenize=tokenize,\n",
    "    pad_token=TOKENIZER.pad_token_id,\n",
    "    unk_token=TOKENIZER.unk_token_id,\n",
    "    init_token=TOKENIZER.bos_token_id,\n",
    "    eos_token=TOKENIZER.eos_token_id,\n",
    "    lower=False,\n",
    "    batch_first=True,\n",
    "    fix_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9aa4f-fe55-4b70-b779-e6c68610973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset(\"data/wiki2.train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fce297e-f3ba-4322-88e7-15401b41e2a6",
   "metadata": {},
   "source": [
    "creates a BucketIterator for batching and iterating over the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b05e1-1160-4e00-b487-3d60ed23240a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    device=DEVICE,\n",
    "    sort=False,\n",
    "    repeat=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaca9c7-295f-4d0b-a413-b3657f54736e",
   "metadata": {},
   "source": [
    "### Sample Batch\n",
    "\n",
    "Note that our sequence length is 512 and batch size is 32."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ecf7ea-fbb5-498b-9bb4-c64641cab734",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    src = batch.src\n",
    "    print(\"Sample Data (Input IDs):\")\n",
    "    print(src)\n",
    "    print(\"Shape of the input batch:\", src.shape)\n",
    "\n",
    "    print(\"Decoded Text:\")\n",
    "    for j in range(src.size(0)):\n",
    "        decoded_text = TOKENIZER.decode(src[j], skip_special_tokens=True)\n",
    "        print(decoded_text)\n",
    "\n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7fcf08-7acb-4d80-8655-28f75f32cbd5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e0e847-332d-4c03-8d71-6a49a71eb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(vocab_size, d_model=512, N=6, heads=8, dropout=0.1, device=\"cpu\"):\n",
    "    \"\"\"Initialize and return the Transformer model\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        d_model (int): The dimensionality of the model.\n",
    "        N (int): The number of decoder layers.\n",
    "        heads (int): The number of attention heads.\n",
    "        dropout (float): The dropout rate.\n",
    "\n",
    "    Returns:\n",
    "        Transformer: The initialized Transformer model.\n",
    "    \"\"\"\n",
    "    model = Transformer(vocab_size, d_model, N, heads, dropout, device)\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e472c-ce38-4e84-a23f-3c83b1d5f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_iterator, optimizer, device, epochs, print_every=50):\n",
    "    model.train()\n",
    "    start = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "\n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            src = batch.src.to(device)\n",
    "            src_mask = create_src_mask(src, TOKENIZER.pad_token_id, device)\n",
    "            no_peak_mask = nopeak_mask(src.size(1), device)\n",
    "\n",
    "            # https://github.com/SamLynnEvans/Transformer/blob/master/Batch.py#L20\n",
    "            preds = model(src, src_mask & no_peak_mask)\n",
    "            some_src = src\n",
    "\n",
    "            # Ensure that predictions and targets are correctly aligned\n",
    "            preds = preds[:, :-1, :]  # exclude the last time step predictions\n",
    "            ys = src[:, 1:].contiguous().view(-1)  # Targets shifted by one position\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(\n",
    "                preds.reshape(-1, preds.size(-1)),\n",
    "                ys,\n",
    "                ignore_index=TOKENIZER.pad_token_id,\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % print_every == 0:\n",
    "                p = int(100 * (i + 1) / len(train_iterator))\n",
    "                avg_loss = total_loss / print_every\n",
    "                print(\n",
    "                    f\"{(time.time() - start) // 60}m: epoch {epoch + 1} [{p}%]  loss = {avg_loss:.3f}\",\n",
    "                    end=\"\\r\",\n",
    "                )\n",
    "                total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9027dc-e704-4f75-97ac-4065b65c7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    device=DEVICE,\n",
    "    sort=False,\n",
    "    repeat=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00081d-96ee-4586-bdab-e00edba8a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(TOKENIZER)\n",
    "model = get_model(vocab_size, d_model=512, N=6, heads=8, dropout=0.1, device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb2310-55a9-4816-aff7-5b8fcdfb2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(model, train_iterator, optimizer, device=DEVICE, epochs=20, print_every=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588ad69c-2b3a-4505-bbe9-688501412a7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
