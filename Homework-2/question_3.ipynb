{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1608669c-eb69-426c-be99-15c9d9812965",
   "metadata": {},
   "source": [
    "# Question 3: Cosine Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4a4d68-5b42-4fe0-8006-7e0c7f810a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchtext.data as data\n",
    "from models.attention import MultiHeadAttention\n",
    "from models.embedder import Embedder, FeedForward, Norm, PositionalEncoder, get_clones\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01dae61-429f-4f97-bf53-7bd920faa875",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadaf4e6-2e1b-4303-9a96-1966004d9ae6",
   "metadata": {},
   "source": [
    "## Cosine Similarity Attention\n",
    "\n",
    "* We're using cosine similarity as our new distance metric:\n",
    "\n",
    "$\\text{Cosim} = \\frac{Q \\cdot K^T}{\\|Q\\| \\|K\\|}$\n",
    "\n",
    "$\\text{Scores} = \\frac{\\text{Cosim}}{\\sqrt{d_k}}$\n",
    "\n",
    "* We're using $\\sqrt{d_k}$ to avoid sharp updates due to softmax.\n",
    "* Cosine similarity focuses only on the orientation of the vectors, not their magnitude.\n",
    "* The only major change here is that we're normalizing the vectors to unit length before taking their dot products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c2c92-1bb5-42ae-993c-7854fca4460a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_attention(q, k, v, d_k, mask=None, dropout=None):\n",
    "    # Normalize q and k to unit vectors\n",
    "    q_norm = q / (q.norm(dim=-1, keepdim=True) + 1e-9)\n",
    "    k_norm = k / (k.norm(dim=-1, keepdim=True) + 1e-9)\n",
    "\n",
    "    # cosim\n",
    "    scores = torch.matmul(q_norm, k_norm.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "\n",
    "    if mask is not None:\n",
    "        mask = mask.unsqueeze(1)\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        scores = dropout(scores)\n",
    "\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecb82a3-06b0-49df-92ce-8b0ad93be2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CosineMultiHeadAttention(MultiHeadAttention):\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        bs = q.size(0)\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        scores = cosine_similarity_attention(q, k, v, self.d_k, mask, self.dropout)\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "        output = self.out(concat)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86df5176-9027-4f32-a095-c9cf374eea42",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "We're using `EuclideanMultiHeadAttention` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5104169e-9892-46e8-a62b-5ef1bab5e20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.norm_1 = Norm(d_model)\n",
    "        self.dropout_1 = nn.Dropout(dropout)\n",
    "        self.attn_1 = CosineMultiHeadAttention(heads, d_model, dropout=dropout)\n",
    "        self.ff = FeedForward(d_model, dropout=dropout)\n",
    "        self.norm_2 = Norm(d_model)\n",
    "        self.dropout_2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, trg_mask):\n",
    "        x2 = self.norm_1(x)\n",
    "        x = x + self.dropout_1(self.attn_1(x2, x2, x2, trg_mask))\n",
    "        x2 = self.norm_2(x)\n",
    "        x = x + self.dropout_2(self.ff(x2))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c29c7b6-9ede-4de2-92bf-01b772a7c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout):\n",
    "        super().__init__()\n",
    "        self.embed = Embedder(vocab_size, d_model)\n",
    "        self.pe = PositionalEncoder(d_model, dropout=dropout)\n",
    "        self.layers = get_clones(DecoderLayer(d_model, heads, dropout), N)\n",
    "        self.norm = Norm(d_model)\n",
    "\n",
    "    def forward(self, trg, trg_mask):\n",
    "        x = self.embed(trg)\n",
    "        x = self.pe(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, trg_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185e24f7-c328-47a5-b169-c7a6c9310e3e",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76eedc1-1510-4165-8dd0-b55df5b7805e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, N, heads, dropout, device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.vocab_size = vocab_size\n",
    "        self.decoder = Decoder(vocab_size, d_model, N, heads, dropout)\n",
    "        self.out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, trg, trg_mask):\n",
    "        d_output = self.decoder(trg, trg_mask)\n",
    "        output = self.out(d_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9ab07d-4c13-4869-b51e-45951d93f764",
   "metadata": {},
   "source": [
    "## No-Peak Mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5209633-9eb5-49bf-bdac-3b1d2fc69593",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nopeak_mask(size, device):\n",
    "    np_mask = torch.triu(torch.ones((1, size, size), device=device), 1).bool()\n",
    "    return np_mask\n",
    "\n",
    "\n",
    "def create_src_mask(src, pad_token_id, device):\n",
    "    src_mask = (src != pad_token_id).unsqueeze(-2)\n",
    "    return src_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae92f31-78aa-4ccc-9002-bb588cb0f656",
   "metadata": {},
   "source": [
    "## Data Feeder\n",
    "\n",
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d4a125-4183-4c80-9ea4-65cf66a42a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER = GPT2Tokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f318f68-eacb-48a9-beb0-6574b64b00cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER.add_special_tokens(\n",
    "    {\"pad_token\": \"[PAD]\", \"bos_token\": \"<sos>\", \"eos_token\": \"<eos>\"}\n",
    ")\n",
    "TOKENIZER.pad_token_id, TOKENIZER.bos_token_id, TOKENIZER.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c490266-f504-4f40-8bd7-529c9d165cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = TOKENIZER.encode(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,  # sequence length\n",
    "        truncation=True,\n",
    "    )\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4e4188-f0a3-4da0-b0af-89609658bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    examples = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if line.strip():\n",
    "                src_line = line.strip()\n",
    "                examples.append(data.Example.fromlist([src_line], [(\"src\", SRC)]))\n",
    "    dataset = data.Dataset(examples, [(\"src\", SRC)])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62e2a94f-e0b7-4694-a8b1-e366b727b623",
   "metadata": {},
   "source": [
    "### BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ad1ef0-ee50-4ac4-8543-52cd09150224",
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = data.Field(\n",
    "    use_vocab=False,\n",
    "    tokenize=tokenize,\n",
    "    pad_token=TOKENIZER.pad_token_id,\n",
    "    unk_token=TOKENIZER.unk_token_id,\n",
    "    init_token=TOKENIZER.bos_token_id,\n",
    "    eos_token=TOKENIZER.eos_token_id,\n",
    "    lower=False,\n",
    "    batch_first=True,\n",
    "    fix_length=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d9aa4f-fe55-4b70-b779-e6c68610973b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = load_dataset(\"data/wiki2.train.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860be704-5b5b-4a28-b7f2-319bb80f2d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data = load_dataset(\"data/wiki2.valid.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7fcf08-7acb-4d80-8655-28f75f32cbd5",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e0e847-332d-4c03-8d71-6a49a71eb4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(vocab_size, d_model=512, N=6, heads=8, dropout=0.1, device=\"cpu\"):\n",
    "    \"\"\"Initialize and return the Transformer model\n",
    "\n",
    "    Args:\n",
    "        vocab_size (int): The size of the vocabulary.\n",
    "        d_model (int): The dimensionality of the model.\n",
    "        N (int): The number of decoder layers.\n",
    "        heads (int): The number of attention heads.\n",
    "        dropout (float): The dropout rate.\n",
    "\n",
    "    Returns:\n",
    "        Transformer: The initialized Transformer model.\n",
    "    \"\"\"\n",
    "    model = Transformer(vocab_size, d_model, N, heads, dropout, device)\n",
    "    model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168e472c-ce38-4e84-a23f-3c83b1d5f208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    model, train_iterator, valid_iterator, optimizer, device, epochs, print_every=50\n",
    "):\n",
    "    train_losses = []\n",
    "    train_perplexities = []\n",
    "    valid_losses = []\n",
    "    valid_perplexities = []\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        intermediate_train_loss = 0\n",
    "        start = time.time()\n",
    "\n",
    "        for i, batch in enumerate(train_iterator):\n",
    "            src = batch.src.to(device)\n",
    "            src_mask = create_src_mask(src, TOKENIZER.pad_token_id, device)\n",
    "            no_peak_mask = nopeak_mask(src.size(1), device)\n",
    "\n",
    "            # https://github.com/SamLynnEvans/Transformer/blob/master/Batch.py#L20\n",
    "            preds = model(src, src_mask & no_peak_mask)\n",
    "            preds = preds[:, :-1, :]  # exclude the last time step predictions\n",
    "            ys = src[:, 1:].contiguous().view(-1)  # Targets shifted by one position\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = F.cross_entropy(\n",
    "                preds.reshape(-1, preds.size(-1)),\n",
    "                ys,\n",
    "                ignore_index=TOKENIZER.pad_token_id,\n",
    "            )\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            intermediate_train_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % print_every == 0:\n",
    "                p = int(100 * (i + 1) / len(train_iterator))\n",
    "                avg_loss = intermediate_train_loss / print_every\n",
    "                print(\n",
    "                    f\"{(time.time() - start) // 60}m: epoch {epoch + 1} [{p}%] Train loss = {avg_loss:.3f}\",\n",
    "                    end=\"\\r\",\n",
    "                )\n",
    "                intermediate_train_loss = 0\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_iterator)\n",
    "        train_perplexity = torch.exp(torch.tensor(avg_train_loss)).item()\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_perplexities.append(train_perplexity)\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        total_valid_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for i, batch in enumerate(valid_iterator):\n",
    "                src = batch.src.to(device)\n",
    "                src_mask = create_src_mask(src, TOKENIZER.pad_token_id, device)\n",
    "                no_peak_mask = nopeak_mask(src.size(1), device)\n",
    "\n",
    "                preds = model(src, src_mask & no_peak_mask)\n",
    "                preds = preds[:, :-1, :]\n",
    "                ys = src[:, 1:].contiguous().view(-1)\n",
    "\n",
    "                loss = F.cross_entropy(\n",
    "                    preds.reshape(-1, preds.size(-1)),\n",
    "                    ys,\n",
    "                    ignore_index=TOKENIZER.pad_token_id,\n",
    "                )\n",
    "                total_valid_loss += loss.item()\n",
    "\n",
    "        avg_valid_loss = total_valid_loss / len(valid_iterator)\n",
    "        valid_perplexity = torch.exp(torch.tensor(avg_valid_loss)).item()\n",
    "        valid_losses.append(avg_valid_loss)\n",
    "        valid_perplexities.append(valid_perplexity)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}: Train loss = {avg_train_loss:.3f}, Train Perplexity = {train_perplexity:.3f}\"\n",
    "        )\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}: Validation loss = {avg_valid_loss:.3f}, Validation Perplexity = {valid_perplexity:.3f}\"\n",
    "        )\n",
    "\n",
    "    return train_losses, train_perplexities, valid_losses, valid_perplexities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686b1ef6-1193-4acf-bab3-4b57b8b6804a",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9027dc-e704-4f75-97ac-4065b65c7657",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = data.BucketIterator(\n",
    "    train_data,\n",
    "    batch_size=25,\n",
    "    device=DEVICE,\n",
    "    sort=False,\n",
    "    repeat=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b84e903-4dcb-4ff5-9853-cde11fa0b6ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iterator = data.BucketIterator(\n",
    "    valid_data,\n",
    "    batch_size=25,\n",
    "    device=DEVICE,\n",
    "    sort=False,\n",
    "    repeat=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c00081d-96ee-4586-bdab-e00edba8a391",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(TOKENIZER)\n",
    "model = get_model(vocab_size, d_model=512, N=6, heads=8, dropout=0.1, device=DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eb2310-55a9-4816-aff7-5b8fcdfb2e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = train_model(\n",
    "    model, train_iterator, valid_iterator, optimizer, DEVICE, epochs=5, print_every=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df66c211-9238-4f24-b1ae-8225784288e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, train_perplexities, valid_losses, valid_perplexities = results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1230dbf-4c60-45e4-98e5-5ee8b15ff35f",
   "metadata": {},
   "source": [
    "### Training Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075a4185-04c1-40d3-b1b8-dd047486178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3499d23-99ed-413a-9577-28b74e420829",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = list(range(1, len(train_losses) + 1))\n",
    "train_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Epoch\": epochs,\n",
    "        \"Train Loss\": train_losses,\n",
    "        \"Train Perplexity\": train_perplexities,\n",
    "    }\n",
    ")\n",
    "\n",
    "valid_data = pd.DataFrame(\n",
    "    {\n",
    "        \"Epoch\": epochs,\n",
    "        \"Validation Loss\": valid_losses,\n",
    "        \"Validation Perplexity\": valid_perplexities,\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce8ccfd-b606-4075-9c69-44802e7eaaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=train_data, x=\"Epoch\", y=\"Train Loss\")\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7845ed3-940e-41fe-9c28-5db33a387bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=valid_data, x=\"Epoch\", y=\"Validation Loss\")\n",
    "plt.title(\"Validation Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70fcd56-fc62-44c6-adad-48c4bedb8ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=train_data, x=\"Epoch\", y=\"Train Perplexity\")\n",
    "plt.title(\"Training Perplexity Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc098b0-620e-4856-952c-a7194143c39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(data=valid_data, x=\"Epoch\", y=\"Validation Perplexity\")\n",
    "plt.title(\"Validation Perplexity Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Perplexity\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77bc629-83c9-43f6-9e1f-996d848e4f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
