{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b21d3fe-df17-44b9-a7f2-c6f9cf791b1b",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185504a-699c-4cc9-b136-7f2d56a45ef4",
   "metadata": {},
   "source": [
    "## Reading Wiki Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5df107a-78f4-4357-9882-7c164bc76c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/wiki2.train.txt\", \"r\") as file:\n",
    "    wiki_train = file.read()\n",
    "\n",
    "with open(\"data/wiki2.test.txt\", \"r\") as file:\n",
    "    wiki_test = file.read()\n",
    "\n",
    "with open(\"data/wiki2.valid.txt\", \"r\") as file:\n",
    "    wiki_valid = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "870d32ad-bc64-4a62-9c17-2a118bf31966",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n = Valkyria Chronicles III = \\n \\n Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 ,'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 100 characters\n",
    "wiki_train[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de22b65-6b81-4a5e-b5b1-604aa59031a0",
   "metadata": {},
   "source": [
    "## Spacy Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57f0fd67-193d-41a6-b402-1d60bd4df579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from utils.tokenization import chunked_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab0b849d-e9c5-438c-9d93-c215e8c3d0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"xx_ent_wiki_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4807c7ca-c1bb-406f-9f39-7c197688c7b7",
   "metadata": {},
   "source": [
    "This model is a multi-language model trained on Wikipedia, supporting named entity recognition for multiple languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c381ea70-b8b1-47c6-a1c0-e8ea883980f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_train = chunked_tokenization(wiki_train, nlp)\n",
    "spacy_test = chunked_tokenization(wiki_test, nlp)\n",
    "spacy_valid = chunked_tokenization(wiki_valid, nlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b9d5b11-d2a3-477d-a9ec-eafad21eeebc",
   "metadata": {},
   "source": [
    "Before and after tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb8f723-dc1b-467d-8eea-c7724d5e7bb3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' \\n ',\n",
       " '=',\n",
       " 'Valkyria',\n",
       " 'Chronicles',\n",
       " 'III',\n",
       " '=',\n",
       " '\\n \\n ',\n",
       " 'Senjō',\n",
       " 'no',\n",
       " 'Valkyria',\n",
       " '3',\n",
       " ':',\n",
       " '<',\n",
       " 'unk',\n",
       " '>',\n",
       " 'Chronicles',\n",
       " '(',\n",
       " 'Japanese',\n",
       " ':',\n",
       " '戦場のヴァルキュリア3']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_train[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ea87d7-daff-45f6-b7fa-7483541c1fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n = Valkyria Chronicles III = \\n \\n Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 ,'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_train[0:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cc59ea-5084-4ce0-800d-5724feb04f2e",
   "metadata": {},
   "source": [
    "## Pre-trained `GPT2TokenizerFast`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9221d7d-208a-44a5-900c-ca050c8e0797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacob/.pyenv/versions/miniconda3-3.11-24.1.2-0/envs/msai337/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from utils.tokenization import chunked_tokenization_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "62751b6e-6d85-48ab-a54f-1812748ffa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "48060d5c-09a5-4bc5-89a5-f8f4e1fa01fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1134371 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "gpt2_train = chunked_tokenization_gpt2(wiki_train, gpt2_tokenizer)\n",
    "gpt2_valid = chunked_tokenization_gpt2(wiki_valid, gpt2_tokenizer)\n",
    "gpt2_test = chunked_tokenization_gpt2(wiki_test, gpt2_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ace6af73-17a0-44c0-8857-896a291a4022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Ġ',\n",
       " 'Ċ',\n",
       " 'Ġ=',\n",
       " 'ĠV',\n",
       " 'alky',\n",
       " 'ria',\n",
       " 'ĠChronicles',\n",
       " 'ĠIII',\n",
       " 'Ġ=',\n",
       " 'Ġ',\n",
       " 'Ċ',\n",
       " 'Ġ',\n",
       " 'Ċ',\n",
       " 'ĠSen',\n",
       " 'j',\n",
       " 'Åį',\n",
       " 'Ġno',\n",
       " 'ĠV',\n",
       " 'alky',\n",
       " 'ria']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt2_train[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915ec293-b987-473a-8de0-b23ed5981b8d",
   "metadata": {},
   "source": [
    "* `Ġ` indicates a space before the word in the original text (part of GPT-2's byte pair encoding to differentiate between words that start after a space and subwords that occur in the middle of words)\n",
    "* `Ċ` represents a newline character in the text.\n",
    "* Words like \"Valkyria\" and \"Chronicles\" are split into subwords or individual characters (`V`, `alky`, `ria`, `Chronicles`), which are common subword units in the tokenizer's vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c7763c-6a18-4573-a8d6-1c6dcce4ce13",
   "metadata": {},
   "source": [
    "## Differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "961d2a36-d2dc-46fd-9b02-be0ad41c0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "untokenized_test = wiki_test[0:1000].split(\" \")[0:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3d2475e-f59e-4032-8815-4bbee71ecfb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untokenized                    | Spacy Tokens                   | GPT-2 Tokens                  \n",
      "-------------------------------+--------------------------------+-------------------------------\n",
      "                               |  \\n                            | Ġ                             \n",
      "\\n                             | =                              | Ċ                             \n",
      "=                              | Robert                         | Ġ=                            \n",
      "Robert                         | <                              | ĠRobert                       \n",
      "<unk>                          | unk                            | Ġ<                            \n",
      "=                              | >                              | unk                           \n",
      "\\n                             | =                              | >                             \n",
      "\\n                             | \\n \\n                          | Ġ=                            \n",
      "Robert                         | Robert                         | Ġ                             \n",
      "<unk>                          | <                              | Ċ                             \n",
      "is                             | unk                            | Ġ                             \n",
      "an                             | >                              | Ċ                             \n",
      "English                        | is                             | ĠRobert                       \n",
      "film                           | an                             | Ġ<                            \n",
      ",                              | English                        | unk                           \n",
      "television                     | film                           | >                             \n",
      "and                            | ,                              | Ġis                           \n",
      "theatre                        | television                     | Ġan                           \n",
      "actor                          | and                            | ĠEnglish                      \n",
      ".                              | theatre                        | Ġfilm                         \n",
      "He                             | actor                          | Ġ,                            \n",
      "had                            | .                              | Ġtelevision                   \n",
      "a                              | He                             | Ġand                          \n",
      "guest                          | had                            | Ġtheatre                      \n",
      "@-@                            | a                              | Ġactor                        \n",
      "starring                       | guest                          | Ġ.                            \n",
      "role                           | @-@                            | ĠHe                           \n",
      "on                             | starring                       | Ġhad                          \n",
      "the                            | role                           | Ġa                            \n",
      "television                     | on                             | Ġguest                        \n",
      "series                         | the                            | Ġ@                            \n",
      "The                            | television                     | -                             \n",
      "Bill                           | series                         | @                             \n",
      "in                             | The                            | Ġstarring                     \n",
      "2000                           | Bill                           | Ġrole                         \n",
      ".                              | in                             | Ġon                           \n",
      "This                           | 2000                           | Ġthe                          \n",
      "was                            | .                              | Ġtelevision                   \n",
      "followed                       | This                           | Ġseries                       \n",
      "by                             | was                            | ĠThe                          \n",
      "a                              | followed                       | ĠBill                         \n",
      "starring                       | by                             | Ġin                           \n",
      "role                           | a                              | Ġ2000                         \n",
      "in                             | starring                       | Ġ.                            \n",
      "the                            | role                           | ĠThis                         \n",
      "play                           | in                             | Ġwas                          \n",
      "Herons                         | the                            | Ġfollowed                     \n",
      "written                        | play                           | Ġby                           \n",
      "by                             | Herons                         | Ġa                            \n",
      "Simon                          | written                        | Ġstarring                     \n",
      "Stephens                       | by                             | Ġrole                         \n",
      ",                              | Simon                          | Ġin                           \n",
      "which                          | Stephens                       | Ġthe                          \n",
      "was                            | ,                              | Ġplay                         \n",
      "performed                      | which                          | ĠHer                          \n",
      "in                             | was                            | ons                           \n",
      "2001                           | performed                      | Ġwritten                      \n",
      "at                             | in                             | Ġby                           \n",
      "the                            | 2001                           | ĠSimon                        \n",
      "Royal                          | at                             | ĠStephens                     \n",
      "Court                          | the                            | Ġ,                            \n",
      "Theatre                        | Royal                          | Ġwhich                        \n",
      ".                              | Court                          | Ġwas                          \n",
      "He                             | Theatre                        | Ġperformed                    \n",
      "had                            | .                              | Ġin                           \n",
      "a                              | He                             | Ġ2001                         \n",
      "guest                          | had                            | Ġat                           \n",
      "role                           | a                              | Ġthe                          \n",
      "in                             | guest                          | ĠRoyal                        \n",
      "the                            | role                           | ĠCourt                        \n",
      "television                     | in                             | ĠTheatre                      \n",
      "series                         | the                            | Ġ.                            \n",
      "Judge                          | television                     | ĠHe                           \n",
      "John                           | series                         | Ġhad                          \n",
      "<unk>                          | Judge                          | Ġa                            \n",
      "in                             | John                           | Ġguest                        \n",
      "2002                           | <                              | Ġrole                         \n",
      ".                              | unk                            | Ġin                           \n",
      "In                             | >                              | Ġthe                          \n",
      "2004                           | in                             | Ġtelevision                   \n",
      "<unk>                          | 2002                           | Ġseries                       \n",
      "landed                         | .                              | ĠJudge                        \n",
      "a                              | In                             | ĠJohn                         \n",
      "role                           | 2004                           | Ġ<                            \n",
      "as                             | <                              | unk                           \n",
      "                               | unk                            | >                             \n",
      "Craig                          | >                              | Ġin                           \n",
      "                               | landed                         | Ġ2002                         \n",
      "in                             | a                              | Ġ.                            \n",
      "the                            | role                           | ĠIn                           \n",
      "episode                        | as                             | Ġ2004                         \n",
      "                               |                                | Ġ<                            \n",
      "Teddy                          | Craig                          | unk                           \n",
      "s                              |                                | >                             \n",
      "Story                          | in                             | Ġlanded                       \n",
      "                               | the                            | Ġa                            \n",
      "of                             | episode                        | Ġrole                         \n",
      "the                            |                                | Ġas                           \n",
      "television                     | Teddy                          | Ġ                             \n",
      "series                         |                                | ĠCraig                        \n",
      "The                            | s                              | Ġ                             \n",
      "Long                           | Story                          | Ġin                           \n",
      "Firm                           |                                | Ġthe                          \n",
      ";                              | of                             | Ġepisode                      \n",
      "he                             | the                            | Ġ                             \n",
      "starred                        | television                     | ĠTeddy                        \n",
      "alongside                      | series                         | Ġ                             \n",
      "actors                         | The                            | s                             \n",
      "Mark                           | Long                           | ĠStory                        \n",
      "Strong                         | Firm                           | Ġ                             \n",
      "and                            | ;                              | Ġof                           \n",
      "Derek                          | he                             | Ġthe                          \n",
      "Jacobi                         | starred                        | Ġtelevision                   \n",
      ".                              | alongside                      | Ġseries                       \n",
      "He                             | actors                         | ĠThe                          \n",
      "was                            | Mark                           | ĠLong                         \n",
      "cast                           | Strong                         | ĠFirm                         \n",
      "in                             | and                            | Ġ;                            \n",
      "the                            | Derek                          | Ġhe                           \n",
      "2005                           | Jacobi                         | Ġstarred                      \n",
      "theatre                        | .                              | Ġalongside                    \n",
      "productions                    | He                             | Ġactors                       \n",
      "of                             | was                            | ĠMark                         \n",
      "the                            | cast                           | ĠStrong                       \n",
      "Philip                         | in                             | Ġand                          \n",
      "Ridley                         | the                            | ĠDerek                        \n",
      "play                           | 2005                           | ĠJacob                        \n",
      "Mercury                        | theatre                        | i                             \n",
      "Fur                            | productions                    | Ġ.                            \n",
      ",                              | of                             | ĠHe                           \n",
      "which                          | the                            | Ġwas                          \n",
      "was                            | Philip                         | Ġcast                         \n",
      "performed                      | Ridley                         | Ġin                           \n",
      "at                             | play                           | Ġthe                          \n",
      "the                            | Mercury                        | Ġ2005                         \n",
      "Drum                           | Fur                            | Ġtheatre                      \n",
      "Theatre                        | ,                              | Ġproductions                  \n",
      "in                             | which                          | Ġof                           \n",
      "Plymouth                       | was                            | Ġthe                          \n",
      "and                            | performed                      | ĠPhilip                       \n",
      "the                            | at                             | ĠRidley                       \n",
      "<unk>                          | the                            | Ġplay                         \n",
      "<unk>                          | Drum                           | ĠMercury                      \n",
      "Factory                        | Theatre                        | ĠFur                          \n",
      "in                             | in                             | Ġ,                            \n",
      "London                         | Plymouth                       | Ġwhich                        \n",
      ".                              | and                            | Ġwas                          \n",
      "He                             | the                            | Ġperformed                    \n",
      "was                            | <                              | Ġat                           \n",
      "directed                       | unk                            | Ġthe                          \n",
      "by                             | >                              | ĠDrum                         \n",
      "John                           | <                              | ĠTheatre                      \n",
      "<unk>                          | unk                            | Ġin                           \n",
      "and                            | >                              | ĠPlymouth                     \n",
      "starred                        | Factory                        | Ġand                          \n",
      "alongside                      | in                             | Ġthe                          \n",
      "Ben                            | London                         | Ġ<                            \n",
      "<unk>                          | .                              | unk                           \n",
      ",                              | He                             | >                             \n",
      "Shane                          | was                            | Ġ<                            \n",
      "<unk>                          | directed                       | unk                           \n",
      ",                              | by                             | >                             \n",
      "Harry                          | John                           | ĠFactory                      \n",
      "Kent                           | <                              | Ġin                           \n",
      ",                              | unk                            | ĠLondon                       \n",
      "Fraser                         | >                              | Ġ.                            \n",
      "<unk>                          | and                            | ĠHe                           \n",
      ",                              | starred                        | Ġwas                          \n",
      "Sophie                         | alongside                      | Ġdirected                     \n",
      "Stanton                        | Ben                            | Ġby                           \n",
      "and                            | <                              | ĠJohn                         \n",
      "Dominic                        | unk                            | Ġ<                            \n",
      "Hall                           | >                              | unk                           \n",
      ".                              | ,                              | >                             \n",
      "\\n                             | Shane                          | Ġand                          \n",
      "In                             | <                              | Ġstarred                      \n",
      "2006                           | unk                            | Ġalongside                    \n",
      ",                              | >                              | ĠBen                          \n",
      "<unk>                          | ,                              | Ġ<                            \n",
      "starred                        | Harry                          | unk                           \n",
      "alongside                      | Kent                           | >                             \n",
      "<unk>                          | ,                              | Ġ,                            \n",
      "in                             | Fraser                         | ĠShane                        \n",
      "the                            | <                              | Ġ<                            \n",
      "play                           | unk                            | unk                           \n",
      "<unk>                          | >                              | >                             \n",
      "written                        | ,                              | Ġ,                            \n",
      "by                             | Sophie                         | ĠHarry                        \n",
      "Mark                           | Stanton                        | ĠKent                         \n",
      "<unk>                          | and                            | Ġ,                            \n",
      ".                              | Dominic                        | ĠFraser                       \n",
      "He                             | Hall                           | Ġ<                            \n",
      "appeared                       | .                              | unk                           \n",
      "on                             | \\n                             | >                             \n",
      "a                              | In                             | Ġ,                            \n",
      "2006                           | 2006                           | ĠSophie                       \n",
      "episode                        | ,                              | ĠStanton                      \n",
      "of                             | <                              | Ġand                          \n",
      "the                            | unk                            | ĠDominic                      \n",
      "televisio                      | >                              | ĠHall                         \n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Untokenized':<30} | {'Spacy Tokens':<30} | {'GPT-2 Tokens':<30}\")\n",
    "print(f\"{'-'*30}-+-{'-'*30}-+-{'-'*30}\")\n",
    "\n",
    "for i in range(200):\n",
    "    untokenized = repr(untokenized_test[i]) if i < len(untokenized_test) else \"\"\n",
    "    spacy_token = repr(spacy_test[i]) if i < len(spacy_test) else \"\"\n",
    "    gpt2_token = repr(gpt2_test[i]) if i < len(gpt2_test) else \"\"\n",
    "\n",
    "    untokenized = untokenized.strip(\"'\\\"\")\n",
    "    spacy_token = spacy_token.strip(\"'\\\"\")\n",
    "    gpt2_token = gpt2_token.strip(\"'\\\"\")\n",
    "\n",
    "    print(f\"{untokenized:<30} | {spacy_token:<30} | {gpt2_token:<30}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a80603-ab35-4f96-807b-5875dc86d256",
   "metadata": {},
   "source": [
    "Some key differences we can see:\n",
    "\n",
    "1. **Granularity**:\n",
    "    1. Spacy produces more word-like tokens, closely aligning with the actual words and punctuations in the text. This could be because Spacy is designed for tasks that require understanding the text at the word level, such as part-of-speech tagging, entity recognition, and dependency parsing.\n",
    "    2. GPT-2 breaks down the text into subword units, represented as byte-pair encodings. This method captures the internal structure of words, allowing the model to handle a wide range of vocabulary, including neologisms and morphologically rich languages, with a fixed-size vocabulary.\n",
    "2. **Special Characters and Whitespace**:\n",
    "    1. Spacy treats newlines, spaces, and punctuation marks as separate tokens, which can be useful for syntactic parsing and sentence boundary detection.\n",
    "    2. GPT-2 has special tokens like `Ġ` to indicate a new word segment following a space, and `Ċ` for newlines, which helps in retaining the textual structure without needing a large vocabulary for whitespace variations.\n",
    "3. **Unknown Tokens**:\n",
    "    1. Spacy uses `<unk>` to represent unknown or out-of-vocabulary (OOV) tokens, which it cannot parse into known word types.\n",
    "    2. GPT-2 rarely encounters OOV tokens due to its subword tokenization. This allows it to piece together unfamiliar terms from known subword components, which is why we see pieces like `Ġ<` and `unk`.\n",
    "4. **Purpose**:\n",
    "    1. Spacy is optimized for NLP tasks requiring understanding of word forms and syntactic structures in context, e.g., NER, part-of-speech tagging, and dependecy parsing.\n",
    "    2. GPT-2 is designed for language generation and comprehension tasks, where subword units allow for more flexible word representation. This allows it to handle a wide variety of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef08cd0d-67d1-47fa-ac78-2379753b5401",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "\n",
    "## Testing Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7e76a3a5-ffad-4e44-91ca-5ed3a7b2ed34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ngrams.ngrams import get_ngram_model, test_ngram_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58c0de57-8d3b-4e57-a87b-bc17e0ca15a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_train_tokens = [\n",
    "    \"this\",\n",
    "    \"is\",\n",
    "    \"a\",\n",
    "    \"sample\",\n",
    "    \"text\",\n",
    "    \"this\",\n",
    "    \"is\",\n",
    "    \"another\",\n",
    "    \"example\",\n",
    "    \"text\",\n",
    "]\n",
    "# test also contains OOV\n",
    "sample_test_tokens = [\"this\", \"is\", \"a\", \"test\", \"text\"]\n",
    "n = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "505274b4-0a9c-4c2c-af14-fcb55f4195e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_bigram_counts, sample_bi_minus_1_gram_counts = get_ngram_model(\n",
    "    sample_train_tokens, n\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f89cbb8-17ae-4b23-9280-7af7d07c20f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('this', 'is'): 2,\n",
       "         ('is', 'a'): 1,\n",
       "         ('a', 'sample'): 1,\n",
       "         ('sample', 'text'): 1,\n",
       "         ('text', 'this'): 1,\n",
       "         ('is', 'another'): 1,\n",
       "         ('another', 'example'): 1,\n",
       "         ('example', 'text'): 1})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_bigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "74b383cc-5292-46df-a26f-26e927ace316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('this',): 2,\n",
       "         ('is',): 2,\n",
       "         ('text',): 2,\n",
       "         ('a',): 1,\n",
       "         ('sample',): 1,\n",
       "         ('another',): 1,\n",
       "         ('example',): 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_bi_minus_1_gram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "daf296c7-d3e6-4ed9-8cdd-8bdd07cd189f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7\n",
      "Number of OOV instances: 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.91152048559648"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ngram_model(\n",
    "    sample_test_tokens, sample_bigram_counts, sample_bi_minus_1_gram_counts, n\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ea498d-433b-40f0-ac1b-9c2ae9350367",
   "metadata": {},
   "source": [
    "Testing it on Dr Suess data from class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e2864f5-979b-438b-ae88-8461bb98b893",
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_suess_test = [\n",
    "    \"<s>\",\n",
    "    \"I\",\n",
    "    \"am\",\n",
    "    \"Sam\",\n",
    "    \"</s>\",\n",
    "    \"<s>\",\n",
    "    \"Sam\",\n",
    "    \"I\",\n",
    "    \"am\",\n",
    "    \"</s>\",\n",
    "    \"<s>\",\n",
    "    \"I\",\n",
    "    \"do\",\n",
    "    \"not\",\n",
    "    \"like\",\n",
    "    \"green\",\n",
    "    \"eggs\",\n",
    "    \"and\",\n",
    "    \"ham\",\n",
    "    \"</s>\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f680d812-35cb-4adf-b3aa-c6f91e851976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Counter({('<s>', 'I'): 2,\n",
       "          ('I', 'am'): 2,\n",
       "          ('</s>', '<s>'): 2,\n",
       "          ('am', 'Sam'): 1,\n",
       "          ('Sam', '</s>'): 1,\n",
       "          ('<s>', 'Sam'): 1,\n",
       "          ('Sam', 'I'): 1,\n",
       "          ('am', '</s>'): 1,\n",
       "          ('I', 'do'): 1,\n",
       "          ('do', 'not'): 1,\n",
       "          ('not', 'like'): 1,\n",
       "          ('like', 'green'): 1,\n",
       "          ('green', 'eggs'): 1,\n",
       "          ('eggs', 'and'): 1,\n",
       "          ('and', 'ham'): 1,\n",
       "          ('ham', '</s>'): 1}),\n",
       " Counter({('<s>',): 3,\n",
       "          ('I',): 3,\n",
       "          ('</s>',): 3,\n",
       "          ('am',): 2,\n",
       "          ('Sam',): 2,\n",
       "          ('do',): 1,\n",
       "          ('not',): 1,\n",
       "          ('like',): 1,\n",
       "          ('green',): 1,\n",
       "          ('eggs',): 1,\n",
       "          ('and',): 1,\n",
       "          ('ham',): 1}))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_ngram_model(dr_suess_test, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef95d01-fb90-4e14-8039-ad492cefd323",
   "metadata": {},
   "source": [
    "$P((<s>\\cap I)|<s>) = $\n",
    "\n",
    "```python\n",
    "('<s>'): 3\n",
    "('<s>', 'I'): 2\n",
    "```\n",
    "\n",
    "$2/3 \\approx 0.67$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e11caac-9e57-4e77-bca6-7f2c64f6692f",
   "metadata": {},
   "source": [
    "## Training and Testing n-gram models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f70d31e-f09f-4fb4-839c-e5f3111392b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ngrams.ngrams import calculate_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63e9905b-2883-43d6-a76b-852ccc9d6d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT-2 vocab size: 27103\n"
     ]
    }
   ],
   "source": [
    "print(\"GPT-2 vocab size:\", len(set(gpt2_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d88fe395-1e9e-4fc5-be13-bdd8ed04746d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 27103\n",
      "Number of OOV instances: 0\n",
      "Vocabulary Size: 27103\n",
      "Number of OOV instances: 47727\n",
      "Vocabulary Size: 602318\n",
      "Number of OOV instances: 138553\n",
      "Vocabulary Size: 2253078\n",
      "Number of OOV instances: 274857\n",
      "GPT-2 Perplexities:\n",
      "{'1-gram': 706.4458638503493, '2-gram': 170.32919159928596, '3-gram': 4365.838282596132, '7-gram': 1137052.2144194297}\n"
     ]
    }
   ],
   "source": [
    "gpt2_perplexities = calculate_perplexities(gpt2_train, gpt2_test)\n",
    "print(\"GPT-2 Perplexities:\")\n",
    "print(gpt2_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ccea9e02-774d-4bc3-8a94-2d5834abb1ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpaCy vocab size: 33240\n"
     ]
    }
   ],
   "source": [
    "print(\"SpaCy vocab size:\", len(set(spacy_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fd9c0a9c-b619-4831-ae8f-a92caf3698cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33240\n",
      "Number of OOV instances: 0\n",
      "Vocabulary Size: 33240\n",
      "Number of OOV instances: 50345\n",
      "Vocabulary Size: 619631\n",
      "Number of OOV instances: 139466\n",
      "Vocabulary Size: 2099033\n",
      "Number of OOV instances: 263516\n",
      "SpaCy Perplexities:\n",
      "{'1-gram': 684.212695073679, '2-gram': 243.0552487411126, '3-gram': 6843.949376282952, '7-gram': 1363436.7859243387}\n"
     ]
    }
   ],
   "source": [
    "spacy_perplexities = calculate_perplexities(spacy_train, spacy_test)\n",
    "print(\"SpaCy Perplexities:\")\n",
    "print(spacy_perplexities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485de637-6c77-495b-961e-430ac3d8c04d",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "* SpaCy has a larger vocabulary size than GPT-2.\n",
    "    * This could be a reason for its higher perplexity, especially in higher n-grams.\n",
    "    * SpaCy may also have more unique tokens and hence higher perplexity, reflecting the model's struggle to predict less frequent or more diverse sequences of words.\n",
    "    * A larger vocabulary can lead to more sparse data distributions (especially in higher n-grams), making accurate predictions more difficult.\n",
    "* uni-gram:\n",
    "    * relatively low for both GPT-2 and SpaCy\n",
    "    * GPT-2 has a slightly higher perplexity\n",
    "    * both models have a good grasp of the single-word distribution in the Wiki-data corpus\n",
    "    * suggests that SpaCy's tokenization method results in a distribution of tokens that slightly better reflects the test corpus.\n",
    "* bi-gram:\n",
    "    * GPT-2 shows a much lower perplexity compared to SpaCy\n",
    "    * suggests that GPT-2's tokenization aligns better with common two-word sequences in the Wiki-data\n",
    "    * or GPT-2 is more effective at capturing the syntactic structure of the \"Wiki-data language\"\n",
    "* tri-gram and 7-gram:\n",
    "    * As we move to higher n-grams, the perplexity increases dramatically for both models, but it's much more pronounced for SpaCy.\n",
    "    * This increase is expected because higher n-grams are less frequent and the model has less information about these longer sequences in the training data, making accurate predictions harder.\n",
    "    * the significantly higher perplexity for SpaCy suggests that its tokenization method might result in less coherent or less frequent n-grams in the context of Wiki-data.\n",
    "    * or SpaCy might be less effective at capturing the language's structure over longer sequences.\n",
    "    * This is apparent with the number of OOV instances we encounter with 7-grams compared to unigrams.\n",
    "* Overall, GPT-2 seems to be more effective at capturing the n-gram patterns of the Wiki-data corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537d5050-eb30-4f0d-853c-b11f5fbf4c36",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "\n",
    "## Adding LaPlace Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5d40515-821f-43f3-84c3-2ad449214db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ngrams.laplace_ngrams import calculate_laplace_perplexities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35ab9e81-fb1a-46a3-97d8-611bc13f9ebb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 27103\n",
      "Number of OOV instances: 0\n",
      "Vocabulary Size: 27103\n",
      "Number of OOV instances: 47727\n",
      "Vocabulary Size: 602318\n",
      "Number of OOV instances: 138553\n",
      "Vocabulary Size: 2253078\n",
      "Number of OOV instances: 274857\n",
      "GPT-2 Perplexities:\n",
      "{'1-gram': 707.6526358520499, '2-gram': 591.5902646528112, '3-gram': 60813.012878479945, '7-gram': 1795022.9119593713}\n"
     ]
    }
   ],
   "source": [
    "gpt2_perplexities = calculate_laplace_perplexities(gpt2_train, gpt2_test)\n",
    "print(\"GPT-2 Perplexities:\")\n",
    "print(gpt2_perplexities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fa956035-4417-4480-8588-34d68434beb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 33240\n",
      "Number of OOV instances: 0\n",
      "Vocabulary Size: 33240\n",
      "Number of OOV instances: 50345\n",
      "Vocabulary Size: 619631\n",
      "Number of OOV instances: 139466\n",
      "Vocabulary Size: 2099033\n",
      "Number of OOV instances: 263516\n",
      "SpaCy Perplexities:\n",
      "{'1-gram': 686.648104026337, '2-gram': 863.2694763969148, '3-gram': 81545.93484314007, '7-gram': 1838737.680698908}\n"
     ]
    }
   ],
   "source": [
    "spacy_perplexities = calculate_laplace_perplexities(spacy_train, spacy_test)\n",
    "print(\"SpaCy Perplexities:\")\n",
    "print(spacy_perplexities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969ae716-327b-4597-88ab-a71f7f7624b3",
   "metadata": {},
   "source": [
    "**Comments:**\n",
    "\n",
    "* GPT-2 still performs consistently better than SpaCy after LaPlace smoothing.\n",
    "* uni-gram: perplexities improved for both models after smoothing\n",
    "* bi-gram:\n",
    "    * this worsened (i.e. increased for both models after smoothing)\n",
    "    * it indicates that the smoothing had a larger impact due to previously unseen bigrams now having a non-zero probability\n",
    "    * this increased is more pronounced for the GPT-2 model, possibly due to the smaller vocab size\n",
    "* 3-gram and 7-gram:\n",
    "    * Substantially increased for both models.\n",
    "    * The increase is dramatic, indicating that with smoothing, the model is penalized more for unseen or rare n-grams, which are more common in higher-order n-grams.\n",
    "* It becomes more apparent that the perplexity might be highly dependent on the probability we assign to the missing tokens.\n",
    "    * If we assign a low probability for OOV words, it results in a high perplexity.\n",
    "    * This is why we see a high error in n-gram models without smoothing as our OOV probability is calculated as:\n",
    "      $$\\textbf{P}_n(\\text{OOV}) = \\frac{\\varepsilon}{c(w_{i-1}) + \\varepsilon \\cdot |V|}$$\n",
    "    * Since $c(w_{i}, w_{i-1}) = 0$. Furthermore, when $c(w_{i-1}) = 0$, this corresonds to $\\frac{1}{|V|}$\n",
    "    * In our case $\\varepsilon = 10^{-3}$ and $\\log(10^{-3}) \\approx -6.9$, which is a poor score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2091f6f6-64f1-4bf2-8f2b-d61c7992e7cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
