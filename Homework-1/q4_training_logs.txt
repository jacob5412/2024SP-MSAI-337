04/07/2024 22:12:04 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: False, 16-bits training: False
04/07/2024 22:12:04 - INFO - __main__ - Training/evaluation parameters TrainingArguments(
_n_gpu=1,
accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'gradient_accumulation_kwargs': None},
adafactor=False,
adam_beta1=0.9,
adam_beta2=0.999,
adam_epsilon=1e-08,
auto_find_batch_size=False,
bf16=False,
bf16_full_eval=False,
data_seed=None,
dataloader_drop_last=False,
dataloader_num_workers=0,
dataloader_persistent_workers=False,
dataloader_pin_memory=True,
dataloader_prefetch_factor=None,
ddp_backend=None,
ddp_broadcast_buffers=None,
ddp_bucket_cap_mb=None,
ddp_find_unused_parameters=None,
ddp_timeout=1800,
debug=[],
deepspeed=None,
disable_tqdm=False,
dispatch_batches=None,
do_eval=True,
do_predict=False,
do_train=True,
eval_accumulation_steps=None,
eval_delay=0,
eval_steps=None,
evaluation_strategy=no,
fp16=False,
fp16_backend=auto,
fp16_full_eval=False,
fp16_opt_level=O1,
fsdp=[],
fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},
fsdp_min_num_params=0,
fsdp_transformer_layer_cls_to_wrap=None,
full_determinism=False,
gradient_accumulation_steps=1,
gradient_checkpointing=False,
gradient_checkpointing_kwargs=None,
greater_is_better=None,
group_by_length=False,
half_precision_backend=auto,
hub_always_push=False,
hub_model_id=None,
hub_private_repo=False,
hub_strategy=every_save,
hub_token=<HUB_TOKEN>,
ignore_data_skip=False,
include_inputs_for_metrics=False,
include_num_input_tokens_seen=False,
include_tokens_per_second=False,
jit_mode_eval=False,
label_names=None,
label_smoothing_factor=0.0,
learning_rate=5e-05,
length_column_name=length,
load_best_model_at_end=False,
local_rank=0,
log_level=passive,
log_level_replica=warning,
log_on_each_node=True,
logging_dir=models/gpt2/runs/Apr07_22-12-04_ip-172-31-11-11,
logging_first_step=False,
logging_nan_inf_filter=True,
logging_steps=500,
logging_strategy=steps,
lr_scheduler_kwargs={},
lr_scheduler_type=linear,
max_grad_norm=1.0,
max_steps=-1,
metric_for_best_model=None,
mp_parameters=,
neftune_noise_alpha=None,
no_cuda=False,
num_train_epochs=3.0,
optim=adamw_torch,
optim_args=None,
optim_target_modules=None,
output_dir=models/gpt2/,
overwrite_output_dir=False,
past_index=-1,
per_device_eval_batch_size=4,
per_device_train_batch_size=4,
prediction_loss_only=False,
push_to_hub=False,
push_to_hub_model_id=None,
push_to_hub_organization=None,
push_to_hub_token=<PUSH_TO_HUB_TOKEN>,
ray_scope=last,
remove_unused_columns=True,
report_to=[],
resume_from_checkpoint=None,
run_name=models/gpt2/,
save_on_each_node=False,
save_only_model=False,
save_safetensors=True,
save_steps=500,
save_strategy=steps,
save_total_limit=None,
seed=42,
skip_memory_metrics=True,
split_batches=None,
tf32=None,
torch_compile=False,
torch_compile_backend=None,
torch_compile_mode=None,
torchdynamo=None,
tpu_metrics_debug=False,
tpu_num_cores=None,
use_cpu=False,
use_ipex=False,
use_legacy_prediction_loop=False,
use_mps_device=False,
warmup_ratio=0.0,
warmup_steps=0,
weight_decay=0.0,
)
Using custom data configuration default-72bf3cf447078dce
04/07/2024 22:12:04 - INFO - datasets.builder - Using custom data configuration default-72bf3cf447078dce
Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text
04/07/2024 22:12:04 - INFO - datasets.info - Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/text
Overwrite dataset info from restored data version if exists.
04/07/2024 22:12:04 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.
Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34
04/07/2024 22:12:04 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34
Found cached dataset text (/home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)
04/07/2024 22:12:04 - INFO - datasets.builder - Found cached dataset text (/home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34)
Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34
04/07/2024 22:12:04 - INFO - datasets.info - Loading Dataset info from /home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34
[INFO|configuration_utils.py:726] 2024-04-07 22:12:04,915 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json
[INFO|configuration_utils.py:789] 2024-04-07 22:12:04,916 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|configuration_utils.py:726] 2024-04-07 22:12:04,949 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json
[INFO|configuration_utils.py:789] 2024-04-07 22:12:04,950 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|tokenization_utils_base.py:2087] 2024-04-07 22:12:04,959 >> loading file vocab.json from cache at /home/ubuntu/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/vocab.json
[INFO|tokenization_utils_base.py:2087] 2024-04-07 22:12:04,959 >> loading file merges.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/merges.txt
[INFO|tokenization_utils_base.py:2087] 2024-04-07 22:12:04,960 >> loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer.json
[INFO|tokenization_utils_base.py:2087] 2024-04-07 22:12:04,960 >> loading file added_tokens.json from cache at None
[INFO|tokenization_utils_base.py:2087] 2024-04-07 22:12:04,960 >> loading file special_tokens_map.json from cache at None
[INFO|tokenization_utils_base.py:2087] 2024-04-07 22:12:04,960 >> loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/tokenizer_config.json
[INFO|configuration_utils.py:726] 2024-04-07 22:12:04,960 >> loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/config.json
[INFO|configuration_utils.py:789] 2024-04-07 22:12:04,961 >> Model config GPT2Config {
  "_name_or_path": "gpt2",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 768,
  "n_head": 12,
  "n_inner": null,
  "n_layer": 12,
  "n_positions": 1024,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.40.0.dev0",
  "use_cache": true,
  "vocab_size": 50257
}

[INFO|modeling_utils.py:3423] 2024-04-07 22:12:05,077 >> loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/model.safetensors
[INFO|configuration_utils.py:928] 2024-04-07 22:12:05,083 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

[INFO|modeling_utils.py:4164] 2024-04-07 22:12:05,218 >> All model checkpoint weights were used when initializing GPT2LMHeadModel.

[INFO|modeling_utils.py:4172] 2024-04-07 22:12:05,218 >> All the weights of GPT2LMHeadModel were initialized from the model checkpoint at gpt2.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
[INFO|configuration_utils.py:883] 2024-04-07 22:12:05,247 >> loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--gpt2/snapshots/607a30d783dfa663caf39e06633721c8d4cfcd7e/generation_config.json
[INFO|configuration_utils.py:928] 2024-04-07 22:12:05,247 >> Generate config GenerationConfig {
  "bos_token_id": 50256,
  "eos_token_id": 50256
}

Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-97e0766824004f01.arrow
04/07/2024 22:12:05 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-97e0766824004f01.arrow
Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-4a2380c709a29071.arrow
04/07/2024 22:12:05 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-4a2380c709a29071.arrow
Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-ede40bd7c754ee59.arrow
04/07/2024 22:12:05 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-ede40bd7c754ee59.arrow
Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-ab83d3f06455c380.arrow
04/07/2024 22:12:05 - INFO - datasets.arrow_dataset - Loading cached processed dataset at /home/ubuntu/.cache/huggingface/datasets/text/default-72bf3cf447078dce/0.0.0/c4a140d10f020282918b5dd1b8a49f0104729c6177f60a6b49ec2a365ec69f34/cache-ab83d3f06455c380.arrow
[INFO|trainer.py:2037] 2024-04-07 22:12:06,101 >> ***** Running training *****
[INFO|trainer.py:2038] 2024-04-07 22:12:06,101 >>   Num examples = 2,373
[INFO|trainer.py:2039] 2024-04-07 22:12:06,101 >>   Num Epochs = 3
[INFO|trainer.py:2040] 2024-04-07 22:12:06,101 >>   Instantaneous batch size per device = 4
[INFO|trainer.py:2043] 2024-04-07 22:12:06,101 >>   Total train batch size (w. parallel, distributed & accumulation) = 4
[INFO|trainer.py:2044] 2024-04-07 22:12:06,101 >>   Gradient Accumulation steps = 1
[INFO|trainer.py:2045] 2024-04-07 22:12:06,101 >>   Total optimization steps = 1,782
[INFO|trainer.py:2046] 2024-04-07 22:12:06,102 >>   Number of trainable parameters = 124,439,808
{'loss': 3.015, 'grad_norm': 2.88812255859375, 'learning_rate': 3.5970819304152635e-05, 'epoch': 0.84}
 28%|███████████▏                            | 500/1782 [10:33<27:07,  1.27s/it][INFO|trainer.py:3296] 2024-04-07 22:22:39,562 >> Saving model checkpoint to models/gpt2/checkpoint-500
[INFO|configuration_utils.py:471] 2024-04-07 22:22:39,563 >> Configuration saved in models/gpt2/checkpoint-500/config.json
[INFO|configuration_utils.py:697] 2024-04-07 22:22:39,563 >> Configuration saved in models/gpt2/checkpoint-500/generation_config.json
[INFO|modeling_utils.py:2584] 2024-04-07 22:22:40,528 >> Model weights saved in models/gpt2/checkpoint-500/model.safetensors
[INFO|tokenization_utils_base.py:2488] 2024-04-07 22:22:40,528 >> tokenizer config file saved in models/gpt2/checkpoint-500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2497] 2024-04-07 22:22:40,529 >> Special tokens file saved in models/gpt2/checkpoint-500/special_tokens_map.json
{'loss': 2.8399, 'grad_norm': 2.6221063137054443, 'learning_rate': 2.1941638608305277e-05, 'epoch': 1.68}
 56%|█████████████████████▉                 | 1000/1782 [21:09<16:32,  1.27s/it][INFO|trainer.py:3296] 2024-04-07 22:33:15,702 >> Saving model checkpoint to models/gpt2/checkpoint-1000
[INFO|configuration_utils.py:471] 2024-04-07 22:33:15,703 >> Configuration saved in models/gpt2/checkpoint-1000/config.json
[INFO|configuration_utils.py:697] 2024-04-07 22:33:15,704 >> Configuration saved in models/gpt2/checkpoint-1000/generation_config.json
[INFO|modeling_utils.py:2584] 2024-04-07 22:33:16,667 >> Model weights saved in models/gpt2/checkpoint-1000/model.safetensors
[INFO|tokenization_utils_base.py:2488] 2024-04-07 22:33:16,667 >> tokenizer config file saved in models/gpt2/checkpoint-1000/tokenizer_config.json
[INFO|tokenization_utils_base.py:2497] 2024-04-07 22:33:16,668 >> Special tokens file saved in models/gpt2/checkpoint-1000/special_tokens_map.json
{'loss': 2.7735, 'grad_norm': 2.45489501953125, 'learning_rate': 7.912457912457913e-06, 'epoch': 2.53}
 84%|████████████████████████████████▊      | 1500/1782 [31:46<05:58,  1.27s/it][INFO|trainer.py:3296] 2024-04-07 22:43:52,143 >> Saving model checkpoint to models/gpt2/checkpoint-1500
[INFO|configuration_utils.py:471] 2024-04-07 22:43:52,145 >> Configuration saved in models/gpt2/checkpoint-1500/config.json
[INFO|configuration_utils.py:697] 2024-04-07 22:43:52,145 >> Configuration saved in models/gpt2/checkpoint-1500/generation_config.json
[INFO|modeling_utils.py:2584] 2024-04-07 22:43:53,092 >> Model weights saved in models/gpt2/checkpoint-1500/model.safetensors
[INFO|tokenization_utils_base.py:2488] 2024-04-07 22:43:53,092 >> tokenizer config file saved in models/gpt2/checkpoint-1500/tokenizer_config.json
[INFO|tokenization_utils_base.py:2497] 2024-04-07 22:43:53,092 >> Special tokens file saved in models/gpt2/checkpoint-1500/special_tokens_map.json
100%|███████████████████████████████████████| 1782/1782 [37:45<00:00,  1.01it/s][INFO|trainer.py:2305] 2024-04-07 22:49:51,639 >> 

Training completed. Do not forget to share your model on huggingface.co/models =)


{'train_runtime': 2265.5761, 'train_samples_per_second': 3.142, 'train_steps_per_second': 0.787, 'train_loss': 2.857445371137591, 'epoch': 3.0}
100%|███████████████████████████████████████| 1782/1782 [37:45<00:00,  1.27s/it]
[INFO|trainer.py:3296] 2024-04-07 22:49:51,679 >> Saving model checkpoint to models/gpt2/
[INFO|configuration_utils.py:471] 2024-04-07 22:49:51,680 >> Configuration saved in models/gpt2/config.json
[INFO|configuration_utils.py:697] 2024-04-07 22:49:51,680 >> Configuration saved in models/gpt2/generation_config.json
[INFO|modeling_utils.py:2584] 2024-04-07 22:49:52,615 >> Model weights saved in models/gpt2/model.safetensors
[INFO|tokenization_utils_base.py:2488] 2024-04-07 22:49:52,616 >> tokenizer config file saved in models/gpt2/tokenizer_config.json
[INFO|tokenization_utils_base.py:2497] 2024-04-07 22:49:52,616 >> Special tokens file saved in models/gpt2/special_tokens_map.json
***** train metrics *****
  epoch                    =        3.0
  train_loss               =     2.8574
  train_runtime            = 0:37:45.57
  train_samples            =       2373
  train_samples_per_second =      3.142
  train_steps_per_second   =      0.787
04/07/2024 22:49:52 - INFO - __main__ - *** Evaluate ***
[INFO|trainer.py:3607] 2024-04-07 22:49:52,660 >> ***** Running Evaluation *****
[INFO|trainer.py:3609] 2024-04-07 22:49:52,660 >>   Num examples = 249
[INFO|trainer.py:3612] 2024-04-07 22:49:52,660 >>   Batch size = 4
100%|███████████████████████████████████████████| 63/63 [00:45<00:00,  1.37it/s]
***** eval metrics *****
  epoch                   =        3.0
  eval_accuracy           =     0.4757
  eval_loss               =     2.6693
  eval_runtime            = 0:00:46.62
  eval_samples            =        249
  eval_samples_per_second =       5.34
  eval_steps_per_second   =      1.351
  perplexity              =    14.4303
[INFO|modelcard.py:450] 2024-04-07 22:50:39,583 >> Dropping the following result as it does not have all the necessary fields:
{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.4757249918540241}]}
