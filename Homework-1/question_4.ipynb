{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9064ef59-89f5-4e8b-8a79-0cfc8b5c0303",
   "metadata": {},
   "source": [
    "# Question 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81d3f23-a749-4585-bab6-5e92d3a3b7c9",
   "metadata": {},
   "source": [
    "## Installing from source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bba2570-ca5c-49ea-9f5d-954b5865eb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# installing hugging face from source\n",
    "# !git clone https://github.com/huggingface/transformers.git\n",
    "# sudo is for AWS (without virtualenv)\n",
    "# !sudo pip3 install -e transformers/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6918756d-f8cc-4a78-ad94-dc88f873087a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a6b10a-e82d-46f6-8627-36e6b26847cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "[{'label': 'POSITIVE', 'score': 0.9998656511306763}]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "!python3 -c \"from transformers import pipeline; print(pipeline('sentiment-analysis')('I love you'))\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deebc987-7589-4014-9b60-c9afc9bddc12",
   "metadata": {},
   "source": [
    "## Fine-Tuning GPT-2 model\n",
    "\n",
    "* Trained using AWS EC2 instance `g4dn.2xlarge`\n",
    "* [Setup Guide](https://towardsdatascience.com/setting-up-pytorch-with-gpu-support-on-ec2-without-preconfigured-amis-3b101b05a765)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc251e7-5ae0-4e09-b136-328a0f8b10e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fafae20a-caf3-49c0-8bc2-dc4f7406819c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !python3 transformers/examples/pytorch/language-modeling/run_clm.py \\\n",
    "#     --model_name_or_path gpt2 \\\n",
    "#     --train_file data/wiki2.train.txt \\\n",
    "#     --validation_file data/wiki2.valid.txt \\\n",
    "#     --per_device_train_batch_size 4 \\\n",
    "#     --per_device_eval_batch_size 4 \\\n",
    "#     --do_train \\\n",
    "#     --do_eval \\\n",
    "#     --output_dir models/gpt2/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb11fa1-a00d-4bec-92a0-c9b8e2464f4e",
   "metadata": {},
   "source": [
    "## Inference using Fine-Tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e29462cb-c3bc-4acc-a8dc-d8891a10e193",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"models/gpt2/\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"models/gpt2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d849e8bd-cf3c-4f34-a056-c61a4673ab26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Tokenizer(name_or_path='models/gpt2/', vocab_size=50257, model_max_length=1024, is_fast=False, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
      "\t50256: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n",
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2Attention(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b932c66-92a6-4f35-a45b-2bcc325e6749",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = \" \\n = Valkyria Chronicles III = \\n \\n Senjō no Valkyria 3 : <unk> Chronicles ( Japanese : 戦場のヴァルキュリア3 ,\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b0502143-3eef-40bc-832b-3de1d6049148",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[  220,   198,   796,   569, 18354,  7496, 17740,  6711,   796,   220,\n",
       "           198,   220,   198,  2311,    73, 13090,   645,   569, 18354,  7496,\n",
       "           513,  1058,  1279,  2954,    29, 17740,   357,  4960,  1058, 10545,\n",
       "           230,    99,   161,   254,   112,  5641, 44444,  9202, 25084, 24440,\n",
       "         12675, 11839,    18,   837]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = tokenizer(test_text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f45dc5a0-a126-4a08-b990-ecb5f270332d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/wiki2.test.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    wiki_test = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "05915e2c-baf4-4d69-95ab-16c6c04c4f95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "device = torch.device(\"cuda\")\n",
    "model.to(device)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2151294e-5f56-4c77-94a5-d39944b86038",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = model.config.n_positions - 1\n",
    "stride = 128\n",
    "n = 0\n",
    "total_loss = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345a81c0-c9d7-4763-a076-bd9768853e9a",
   "metadata": {},
   "source": [
    "Cross-Entropy Loss (H) is defined by:\n",
    "\n",
    "$H = -\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | w_{i-1})$\n",
    "\n",
    "Perplexity can therefore, also be defined by:\n",
    "\n",
    "$P = e^H$\n",
    "\n",
    "$P = e^{-\\frac{1}{N} \\sum_{i=1}^{N} \\log P(w_i | c)}$\n",
    "\n",
    "Since GPT-2 model uses cross-entropy loss, we'll just use the loss from the output.\n",
    "\n",
    "We're taking strides for computational reasons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2ea5539-9882-438a-8358-ac3b06a899a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9805/9805 [03:32<00:00, 46.11it/s]\n"
     ]
    }
   ],
   "source": [
    "# calculate loss for each segment\n",
    "for i in tqdm(range(0, len(wiki_test), stride)):\n",
    "    encoded_chunk = tokenizer.encode(wiki_test[i : i + max_length], return_tensors=\"pt\")\n",
    "\n",
    "    encoded_chunk = encoded_chunk.to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(encoded_chunk, labels=encoded_chunk)\n",
    "        total_loss += outputs.loss.item()\n",
    "        n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3905e2df-53ea-4e5c-9a1e-5c36cae50599",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_loss = total_loss / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd5779ac-4f31-4f2b-8488-c9283229f9a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 17.1722412109375\n"
     ]
    }
   ],
   "source": [
    "perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "print(f\"Perplexity: {perplexity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc07199-d190-404c-9e28-48a2812d5513",
   "metadata": {},
   "source": [
    "* The fine-tuned GPT-2 model shows a perplexity of 17.17, significantly lower than the previous perplexity scores.\n",
    "* The low perplexity of the fine-tuned GPT-2 model suggests it has effectively learned the structure and nuances of the text it was trained on.\n",
    "* Some reasons why the performance is good:\n",
    "    - based on the Transformer architecture, which allows it to effectively capture long-range dependencies in text\n",
    "    - pretrained on a massive corpus of text data\n",
    "    - model utilizes a self-attention mechanism to weigh the importance of each word in the context when predicting the next word\n",
    "    - this model alone has a 124,439,808 trainable parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330328ff-493d-4b61-bc99-b11ed2de5c37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
