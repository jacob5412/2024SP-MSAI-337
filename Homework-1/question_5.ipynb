{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f3af0f9-7af1-4167-a446-f8b860c42605",
   "metadata": {},
   "source": [
    "# Question 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d1cf12-1431-4203-ac84-a669287c5bbb",
   "metadata": {},
   "source": [
    "## Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "670ce08b-10e6-47b1-9e9a-f3d69737f043",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c90992d4-f008-44da-b8ad-3627f75eac88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(\"data/wiki2.train.txt\", \"r\") as file:\n",
    "    wiki_train = file.read()\n",
    "\n",
    "with open(\"data/examples.txt\", \"r\") as file:\n",
    "    example_test = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca07cbd6-ed9b-4974-9ff8-9f1b4038d500",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "split_examples_test = [example.strip() for example in re.split(r\"\\n\\n\", example_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc959416-aad6-4e95-b9b8-0c6add582e61",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "862c4021-8d41-4e3e-ba89-dbab37d23832",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from utils.tokenization import chunked_tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03b580a-cc8b-488e-abad-30f8ed31240e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"xx_ent_wiki_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73b6e753-be19-427f-978c-15acad0e4979",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_train = chunked_tokenization(wiki_train, nlp)\n",
    "\n",
    "spacy_ex_test = []\n",
    "for example in split_examples_test:\n",
    "    spacy_ex_test.append(chunked_tokenization(example, nlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9221d7d-208a-44a5-900c-ca050c8e0797",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jacob/.pyenv/versions/miniconda3-3.11-24.1.2-0/envs/msai337/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from utils.tokenization import chunked_tokenization_gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62751b6e-6d85-48ab-a54f-1812748ffa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt2_tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48060d5c-09a5-4bc5-89a5-f8f4e1fa01fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1134371 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "gpt2_train = chunked_tokenization_gpt2(wiki_train, gpt2_tokenizer)\n",
    "\n",
    "gpt2_test = []\n",
    "for example in split_examples_test:\n",
    "    gpt2_test.append(chunked_tokenization_gpt2(example, gpt2_tokenizer))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d53e6d-e71d-43b0-897f-c1e41cdd46c2",
   "metadata": {},
   "source": [
    "## Testing LaPlace Smoothened Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "caa1674d-c6f1-421d-a1a2-072b56df7081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ngrams.laplace_ngrams import (\n",
    "    calculate_laplace_perplexities,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42f06041-633f-4f71-9356-6569474d2448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Example:\n",
      "['01', '.', 'ĠBest', 'Ġknown', 'Ġfor', 'Ġdeveloping', 'Ġthe', 'Ġtheory', 'Ġof', 'Ġrelativity', ',', 'ĠEinstein', 'Ġalso', 'Ġmade', 'Ġimportant', 'Ġcontributions', 'Ġto', 'Ġquantum', 'Ġmechanics', ',', 'Ġand', 'Ġwas', 'Ġthus', 'Ġa', 'Ġcentral', 'Ġfigure', 'Ġin', 'Ġthe', 'Ġrevolutionary', 'Ġresh', 'aping', 'Ġof', 'Ġthe', 'Ġscientific', 'Ġunderstanding', 'Ġof', 'Ġnature', 'Ġthat', 'Ġmodern', 'Ġphysics', 'Ġaccomplished', 'Ġin', 'Ġthe', 'Ġfirst', 'Ġdecades', 'Ġof', 'Ġthe', 'Ġtwentieth', 'Ġcentury', '.']\n",
      "\n",
      "GPT-2 Perplexities:\n",
      "1-gram: 2227.85\n",
      "2-gram: 4384.27\n",
      "3-gram: 366052.29\n",
      "7-gram: 2217862.66\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['02', '.', 'ĠEveryone', 'Ġlooks', 'Ġto', 'Ġthe', 'ĠStark', 'Ġbanners', ',', 'Ġwith', 'Ġtheir', 'Ġdire', 'wolf', 'Ġcrest', '-', 'of', '-', 'arms', '.', 'ĠWe', 'Ġsee', 'Ġtheir', 'Ġopinions', 'Ġabout', 'Ġthe', 'Ġp', 'ups', 'Ġchange', ',', 'Ġas', 'Ġthey', 'Ġcome', 'Ġto', 'Ġunderstand', 'Ġthe', 'Ġimport', 'Ġof', 'Ġthis', 'Ġo', 'men', '.']\n",
      "\n",
      "GPT-2 Perplexities:\n",
      "1-gram: 4725.23\n",
      "2-gram: 9285.98\n",
      "3-gram: 535953.70\n",
      "7-gram: 2253078.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['03', '.', 'ĠI', 'Ġdo', 'Ġnot', 'Ġlike', 'Ġthem', 'Ġin', 'Ġa', 'Ġhouse', '.', 'ĠI', 'Ġdo', 'Ġnot', 'Ġlike', 'Ġthem', 'Ġwith', 'Ġa', 'Ġmouse', '.', 'ĠI', 'Ġdo', 'Ġnot', 'Ġlike', 'Ġthem', 'Ġhere', 'Ġor', 'Ġthere', '.', 'ĠI', 'Ġdo', 'Ġnot', 'Ġlike', 'Ġthem', 'Ġanywhere', '.', 'ĠI', 'Ġdo', 'Ġnot', 'Ġlike', 'Ġgreen', 'Ġeggs', 'Ġand', 'Ġham', '.', 'ĠI', 'Ġdo', 'Ġnot', 'Ġlike', 'Ġthem', ',', 'ĠSam', '-', 'I', '-', 'am', '.']\n",
      "\n",
      "GPT-2 Perplexities:\n",
      "1-gram: 1667.84\n",
      "2-gram: 5070.12\n",
      "3-gram: 387010.64\n",
      "7-gram: 2253078.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['04', '.', 'ĠI', 'Ġam', 'ĠSam', '.', 'Ġ', 'ĠSam', 'ĠI', 'Ġam', '.', 'Ġ', 'ĠI', 'Ġdo', 'Ġnot', 'Ġlike', 'Ġgreen', 'Ġeggs', 'Ġand', 'Ġham', '.']\n",
      "\n",
      "GPT-2 Perplexities:\n",
      "1-gram: 2275.23\n",
      "2-gram: 8028.19\n",
      "3-gram: 507419.45\n",
      "7-gram: 2253078.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['05', '.', 'ĠI', 'Ġam', 'ĠSam', '.', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠSam', 'ĠI', 'Ġam', '.', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'Ġ', 'ĠI', 'Ġdo', 'Ġnot', 'Ġlike', 'Ġgreen', 'Ġeggs', 'Ġand', 'Ġham', '.']\n",
      "\n",
      "GPT-2 Perplexities:\n",
      "1-gram: 721.15\n",
      "2-gram: 16030.18\n",
      "3-gram: 538322.50\n",
      "7-gram: 2253078.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['06', '.', 'Ġ<', 's', '>', 'ĠI', 'Ġam', 'ĠSam', '.', 'Ġ</', 's', '>', 'Ġ<', 's', '>', 'ĠSam', 'ĠI', 'Ġam', '.', 'Ġ</', 's', '>', 'Ġ<', 's', '>', 'ĠI', 'Ġdo', 'Ġnot', 'Ġlike', 'Ġgreen', 'Ġeggs', 'Ġand', 'Ġham', '.', 'Ġ</', 's', '>']\n",
      "\n",
      "GPT-2 Perplexities:\n",
      "1-gram: 1203.70\n",
      "2-gram: 9565.41\n",
      "3-gram: 538233.01\n",
      "7-gram: 2253078.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['07', '.', 'Ġ<', 's', '>']\n",
      "\n",
      "GPT-2 Perplexities:\n",
      "1-gram: 428.08\n",
      "2-gram: 9031.46\n",
      "3-gram: 602485.95\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['08', '.', 'ĠNatural', 'ĠLanguage', 'ĠProcessing', 'Ġ(', 'N', 'LP', ')', 'Ġis', 'Ġa', 'Ġbranch', 'Ġof', 'Ġartificial', 'Ġintelligence', 'Ġthat', 'Ġfocuses', 'Ġon', 'Ġtechniques', 'Ġthat', 'Ġenable', 'Ġcomputers', 'Ġto', 'Ġunderstand', ',', 'Ġinterpret', 'Ġand', 'Ġmanipulate', 'Ġhuman', 'Ġlanguage', '.']\n",
      "\n",
      "GPT-2 Perplexities:\n",
      "1-gram: 7851.70\n",
      "2-gram: 11695.48\n",
      "3-gram: 497004.80\n",
      "7-gram: 2253078.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['09', '.', 'ĠCommon', 'ĠN', 'LP', 'Ġtasks', 'Ġinclude', 'Ġquestion', 'Ġanswering', ',', 'Ġtext', 'Ġclassification', 'Ġ(', 'including', 'Ġf', 'akes', 'Ġdetection', '),', 'Ġtext', 'Ġsummar', 'ization', ',', 'Ġtext', 'Ġgeneration', 'Ġ(', 'including', 'Ġdialogue', ',', 'Ġtranslation', 'Ġand', 'Ġprogram', 'Ġsynthesis', '),', 'Ġnatural', 'Ġlanguage', 'Ġinference', 'Ġand', 'Ġknowledge', 'base', 'Ġcompletion', ',', 'Ġamong', 'Ġothers', '.']\n",
      "\n",
      "GPT-2 Perplexities:\n",
      "1-gram: 19743.16\n",
      "2-gram: 23496.39\n",
      "3-gram: 602319.07\n",
      "7-gram: 2253078.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['10', '.', 'ĠThe', 'ĠSteelers', ',', 'Ġwhose', 'Ġhistory', 'Ġmay', 'Ġbe', 'Ġtraced', 'Ġto', 'Ġa', 'Ġregional', 'Ġpro', 'Ġteam', 'Ġthat', 'Ġwas', 'Ġestablished', 'Ġin', 'Ġthe', 'Ġearly', 'Ġ1920', 's', ',', 'Ġjoined', 'Ġthe', 'ĠNFL', 'Ġas', 'Ġthe', 'ĠPittsburgh', 'ĠPirates', 'Ġon', 'ĠJuly', 'Ġ8', ',', 'Ġ1933', '.', 'ĠThe', 'Ġteam', 'Ġwas', 'Ġowned', 'Ġby', 'ĠArt', 'ĠRooney', 'Ġand', 'Ġtook', 'Ġits', 'Ġoriginal', 'Ġname', 'Ġfrom', 'Ġthe', 'Ġbaseball', 'Ġteam', 'Ġof', 'Ġthe', 'Ġsame', 'Ġname', ',', 'Ġas', 'Ġwas', 'Ġcommon', 'Ġpractice', 'Ġfor', 'ĠNFL', 'Ġteams', 'Ġat', 'Ġthe', 'Ġtime', '.']\n",
      "\n",
      "GPT-2 Perplexities:\n",
      "1-gram: 1447.01\n",
      "2-gram: 2858.74\n",
      "3-gram: 315272.32\n",
      "7-gram: 2253078.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['11', '.', 'ĠNorthwestern', 'ĠUniversity', 'Ġ(', 'N', 'U', ')', 'Ġis', 'Ġa', 'Ġprivate', 'Ġresearch', 'Ġuniversity', 'Ġin', 'ĠEvan', 'ston', ',', 'ĠIllinois', ',', 'ĠUnited', 'ĠStates', '.', 'ĠEst', 'ablished', 'Ġin', 'Ġ18', '51', 'Ġto', 'Ġserve', 'Ġthe', 'Ġhistoric', 'ĠNorthwest', 'ĠTerritory', ',', 'Ġit', 'Ġis', 'Ġthe', 'Ġoldest', 'Ġchart', 'ered', 'Ġuniversity', 'Ġin', 'ĠIllinois', '.', 'ĠThe', 'Ġuniversity', 'Ġhas', 'Ġits', 'Ġmain', 'Ġcampus', 'Ġalong', 'Ġthe', 'Ġshores', 'Ġof', 'ĠLake', 'ĠMichigan', 'Ġin', 'Ġthe', 'ĠChicago', 'Ġmetropolitan', 'Ġarea', '.']\n",
      "\n",
      "GPT-2 Perplexities:\n",
      "1-gram: 3217.10\n",
      "2-gram: 5137.87\n",
      "3-gram: 432896.55\n",
      "7-gram: 2253078.00\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for gpt2_test_example in gpt2_test:\n",
    "    print(f\"Testing Example:\\n{gpt2_test_example}\\n\")\n",
    "    gpt2_perplexities = calculate_laplace_perplexities(gpt2_train, gpt2_test_example)\n",
    "    print(\"GPT-2 Perplexities:\")\n",
    "    for n_gram, perplexity in gpt2_perplexities.items():\n",
    "        print(f\"{n_gram}: {perplexity:.2f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "875ae399-a29a-4310-9007-b6479091c65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Example:\n",
      "['01', '.', 'Best', 'known', 'for', 'developing', 'the', 'theory', 'of', 'relativity', ',', 'Einstein', 'also', 'made', 'important', 'contributions', 'to', 'quantum', 'mechanics', ',', 'and', 'was', 'thus', 'a', 'central', 'figure', 'in', 'the', 'revolutionary', 'reshaping', 'of', 'the', 'scientific', 'understanding', 'of', 'nature', 'that', 'modern', 'physics', 'accomplished', 'in', 'the', 'first', 'decades', 'of', 'the', 'twentieth', 'century', '.']\n",
      "\n",
      "SpaCy Perplexities:\n",
      "1-gram: 1480.44\n",
      "2-gram: 3564.39\n",
      "3-gram: 311870.04\n",
      "7-gram: 2032440.71\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['02', '.', 'Everyone', 'looks', 'to', 'the', 'Stark', 'banners', ',', 'with', 'their', 'direwolf', 'crest', '-', 'of', '-', 'arms', '.', 'We', 'see', 'their', 'opinions', 'about', 'the', 'pups', 'change', ',', 'as', 'they', 'come', 'to', 'understand', 'the', 'import', 'of', 'this', 'omen', '.']\n",
      "\n",
      "SpaCy Perplexities:\n",
      "1-gram: 2610.69\n",
      "2-gram: 6350.61\n",
      "3-gram: 441522.43\n",
      "7-gram: 2099033.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['03', '.', 'I', 'do', 'not', 'like', 'them', 'in', 'a', 'house', '.', 'I', 'do', 'not', 'like', 'them', 'with', 'a', 'mouse', '.', 'I', 'do', 'not', 'like', 'them', 'here', 'or', 'there', '.', 'I', 'do', 'not', 'like', 'them', 'anywhere', '.', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.', 'I', 'do', 'not', 'like', 'them', ',', 'Sam', '-', 'I', '-', 'am', '.']\n",
      "\n",
      "SpaCy Perplexities:\n",
      "1-gram: 1331.49\n",
      "2-gram: 3128.88\n",
      "3-gram: 353182.02\n",
      "7-gram: 2099033.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['04', '.', 'I', 'am', 'Sam', '.', ' ', 'Sam', 'I', 'am', '.', ' ', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "\n",
      "SpaCy Perplexities:\n",
      "1-gram: 4288.75\n",
      "2-gram: 10353.44\n",
      "3-gram: 451134.04\n",
      "7-gram: 2099033.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['05', '.', 'I', 'am', 'Sam', '.', '      ', 'Sam', 'I', 'am', '.', '      ', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.']\n",
      "\n",
      "SpaCy Perplexities:\n",
      "1-gram: 4802.66\n",
      "2-gram: 10503.61\n",
      "3-gram: 451134.00\n",
      "7-gram: 2099033.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['06', '.', '<', 's', '>', 'I', 'am', 'Sam', '.', '<', '/s', '>', '<', 's', '>', 'Sam', 'I', 'am', '.', '<', '/s', '>', '<', 's', '>', 'I', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.', '<', '/s', '>']\n",
      "\n",
      "SpaCy Perplexities:\n",
      "1-gram: 918.02\n",
      "2-gram: 6678.88\n",
      "3-gram: 543039.77\n",
      "7-gram: 2099033.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['07', '.', '<', 's', '>']\n",
      "\n",
      "SpaCy Perplexities:\n",
      "1-gram: 199.75\n",
      "2-gram: 9165.42\n",
      "3-gram: 620338.53\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['08', '.', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'branch', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'techniques', 'that', 'enable', 'computers', 'to', 'understand', ',', 'interpret', 'and', 'manipulate', 'human', 'language', '.']\n",
      "\n",
      "SpaCy Perplexities:\n",
      "1-gram: 3834.52\n",
      "2-gram: 9159.67\n",
      "3-gram: 418293.91\n",
      "7-gram: 2099033.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['09', '.', 'Common', 'NLP', 'tasks', 'include', 'question', 'answering', ',', 'text', 'classification', '(', 'including', 'fakes', 'detection', ')', ',', 'text', 'summarization', ',', 'text', 'generation', '(', 'including', 'dialogue', ',', 'translation', 'and', 'program', 'synthesis', ')', ',', 'natural', 'language', 'inference', 'and', 'knowledgebase', 'completion', ',', 'among', 'others', '.']\n",
      "\n",
      "SpaCy Perplexities:\n",
      "1-gram: 4521.29\n",
      "2-gram: 14419.80\n",
      "3-gram: 534662.66\n",
      "7-gram: 2099033.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['10', '.', 'The', 'Steelers', ',', 'whose', 'history', 'may', 'be', 'traced', 'to', 'a', 'regional', 'pro', 'team', 'that', 'was', 'established', 'in', 'the', 'early', '1920s', ',', 'joined', 'the', 'NFL', 'as', 'the', 'Pittsburgh', 'Pirates', 'on', 'July', '8', ',', '1933', '.', 'The', 'team', 'was', 'owned', 'by', 'Art', 'Rooney', 'and', 'took', 'its', 'original', 'name', 'from', 'the', 'baseball', 'team', 'of', 'the', 'same', 'name', ',', 'as', 'was', 'common', 'practice', 'for', 'NFL', 'teams', 'at', 'the', 'time', '.']\n",
      "\n",
      "SpaCy Perplexities:\n",
      "1-gram: 974.10\n",
      "2-gram: 1773.87\n",
      "3-gram: 241795.06\n",
      "7-gram: 2099033.00\n",
      "------------------------------\n",
      "Testing Example:\n",
      "['11', '.', 'Northwestern', 'University', '(', 'NU', ')', 'is', 'a', 'private', 'research', 'university', 'in', 'Evanston', ',', 'Illinois', ',', 'United', 'States', '.', 'Established', 'in', '1851', 'to', 'serve', 'the', 'historic', 'Northwest', 'Territory', ',', 'it', 'is', 'the', 'oldest', 'chartered', 'university', 'in', 'Illinois', '.', 'The', 'university', 'has', 'its', 'main', 'campus', 'along', 'the', 'shores', 'of', 'Lake', 'Michigan', 'in', 'the', 'Chicago', 'metropolitan', 'area', '.']\n",
      "\n",
      "SpaCy Perplexities:\n",
      "1-gram: 1693.78\n",
      "2-gram: 3047.57\n",
      "3-gram: 284116.34\n",
      "7-gram: 2099033.00\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "for spacy_test_example in spacy_ex_test:\n",
    "    print(f\"Testing Example:\\n{spacy_test_example}\\n\")\n",
    "    spacy_perplexities = calculate_laplace_perplexities(spacy_train, spacy_test_example)\n",
    "    print(\"SpaCy Perplexities:\")\n",
    "    for n_gram, perplexity in spacy_perplexities.items():\n",
    "        print(f\"{n_gram}: {perplexity:.2f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df3b012-018a-4e0d-a8b4-6f456bdc8618",
   "metadata": {},
   "source": [
    "## Testing Fine-Tuned GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3fcd9ad8-52cc-47d6-a1ca-e1a0e3eb0ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a196c1bb-925e-4185-a0d2-62a9e704940b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained(\"models/gpt2/\")\n",
    "model = GPT2LMHeadModel.from_pretrained(\"models/gpt2/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "faa44eae-24fd-4540-acf0-7bc9be306c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# device = torch.device(\"cuda\")\n",
    "# model.to(device)\n",
    "print(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae4c725e-47e2-4439-871a-dfbaefb11622",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = model.config.n_positions - 1\n",
    "stride = 16\n",
    "n = 0\n",
    "total_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dcac54f-be9c-4dd7-942e-fabc29e6b51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing example: 01. Best known for developing the theory of relati...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "Processing chunks:   0%|                                                                                                      | 0/19 [00:00<?, ?it/s]TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processing chunks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 19/19 [00:07<00:00,  2.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity for the example: 42.22\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing example: 02. Everyone looks to the Stark banners, with thei...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 11/11 [00:03<00:00,  3.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity for the example: 223.58\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing example: 03. I do not like them in a house. I do not like t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 13/13 [00:03<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity for the example: 92.33\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing example: 04. I am Sam.  Sam I am.  I do not like green eggs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:00<00:00,  6.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity for the example: 508.01\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing example: 05. I am Sam.       Sam I am.       I do not like ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:02<00:00,  2.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity for the example: 291.43\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing example: 06. <s> I am Sam. </s> <s> Sam I am. </s> <s> I do...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 6/6 [00:02<00:00,  2.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity for the example: 139.15\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing example: 07. <s>...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:00<00:00,  3.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity for the example: 492.24\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing example: 08. Natural Language Processing (NLP) is a branch ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 12/12 [00:02<00:00,  5.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity for the example: 176.98\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing example: 09. Common NLP tasks include question answering, t...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 17/17 [00:05<00:00,  3.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity for the example: 576.90\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing example: 10. The Steelers, whose history may be traced to a...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:10<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity for the example: 40.07\n",
      "\n",
      "------------------------------------------------------------\n",
      "\n",
      "Processing example: 11. Northwestern University (NU) is a private rese...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing chunks: 100%|█████████████████████████████████████████████████████████████████████████████████████████████| 20/20 [00:08<00:00,  2.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Perplexity for the example: 47.82\n",
      "\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for split_example in split_examples_test:\n",
    "    print(f\"\\nProcessing example: {split_example[:50]}...\")\n",
    "    total_loss = 0\n",
    "    n = 0\n",
    "\n",
    "    for i in tqdm(range(0, len(split_example), stride), desc=\"Processing chunks\"):\n",
    "        encoded_chunk = tokenizer.encode(\n",
    "            split_example[i : i + max_length], return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "        encoded_chunk = encoded_chunk.to(model.device)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            outputs = model(encoded_chunk, labels=encoded_chunk)\n",
    "            total_loss += outputs.loss.item()\n",
    "            n += 1\n",
    "    \n",
    "    average_loss = total_loss / n\n",
    "    perplexity = torch.exp(torch.tensor(average_loss)).item()\n",
    "    print(f\"\\nPerplexity for the example: {perplexity:.2f}\\n\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b1d12a-7b9f-4c40-8655-dd6e6e34abfc",
   "metadata": {},
   "source": [
    "* The GPT-2 model's performance is significantly worser here than it was on wiki2 data.\n",
    "    * the example differs slightly in style or content from the training data.\n",
    "    * the transformer might have memorized the structure of the wiki2 data, which is the performance degraded here\n",
    "* The test set contains a variety of text samples ranging from scientific discussions, literary excerpts, children's books, and sports.\n",
    "* Fine-tuned GPT-2's better performance suggests its ability to generalize across different contexts and styles.\n",
    "* Both SpaCy and GPT-2 n-gram models exhibit very high perplexity scores, particularly for the higher n-grams.\n",
    "    * likely due to the sparse nature of higher-order n-grams\n",
    "    * and the limitation of n-gram models in capturing long-range dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9089d60-040b-4d98-96ec-6c6fce0f6d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
